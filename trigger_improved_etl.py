#!/usr/bin/env python3
"""
ê°œì„ ëœ ETL ë¡œì§ìœ¼ë¡œ ì§ì ‘ ì²˜ë¦¬í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
"""
import asyncio
import logging
from datetime import datetime
from sqlalchemy import text
from database.connection import get_sync_session, get_async_session
from etl.simple_query_executor import SimpleQueryExecutor
from etl.document_transformer import DocumentTransformer

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(name)s - %(message)s')
logger = logging.getLogger(__name__)

async def trigger_improved_etl():
    """ê°œì„ ëœ ETL ë¡œì§ìœ¼ë¡œ ì§ì ‘ ì²˜ë¦¬"""
    anp_seq = 18420
    job_id = "7c9cd4c3-33b0-4403-a426-7e36bd830825"
    
    logger.info(f"ğŸš€ Starting improved ETL processing for anp_seq: {anp_seq}")
    
    try:
        # 1. ì‘ì—… ìƒíƒœë¥¼ processingìœ¼ë¡œ ì—…ë°ì´íŠ¸
        with get_sync_session() as session:
            update_query = """
                UPDATE chat_etl_jobs 
                SET status = 'processing_queries',
                    current_step = 'query_execution',
                    updated_at = CURRENT_TIMESTAMP
                WHERE job_id = :job_id
            """
            session.execute(text(update_query), {"job_id": job_id})
            session.commit()
        
        # 2. Query Execution
        logger.info("ğŸ“Š Step 1: Executing queries...")
        executor = SimpleQueryExecutor()
        results = executor.execute_core_queries(anp_seq)
        
        # ê²°ê³¼ ë³€í™˜
        formatted_results = {}
        for query_name, result in results.items():
            if result.success and result.data:
                formatted_results[query_name] = result.data
            else:
                formatted_results[query_name] = []
        
        logger.info(f"Query results: {len(formatted_results)} queries executed")
        
        # 3. ì‘ì—… ìƒíƒœ ì—…ë°ì´íŠ¸
        with get_sync_session() as session:
            update_query = """
                UPDATE chat_etl_jobs 
                SET status = 'transforming_documents',
                    current_step = 'document_transformation',
                    progress_percentage = 30,
                    updated_at = CURRENT_TIMESTAMP
                WHERE job_id = :job_id
            """
            session.execute(text(update_query), {"job_id": job_id})
            session.commit()
        
        # 4. Document Transformation
        logger.info("ğŸ“ Step 2: Transforming documents...")
        transformer = DocumentTransformer()
        documents = await transformer.transform_all_documents(formatted_results)
        
        # ë¬¸ì„œ íƒ€ì…ë³„ ë¶„í¬ ê³„ì‚°
        doc_types = set()
        doc_type_counts = {}
        for doc in documents:
            doc_types.add(doc.doc_type)
            doc_type_counts[doc.doc_type] = doc_type_counts.get(doc.doc_type, 0) + 1
        
        # 5. ì‘ì—… ì™„ë£Œ ìƒíƒœ ì—…ë°ì´íŠ¸
        with get_sync_session() as session:
            update_query = """
                UPDATE chat_etl_jobs 
                SET status = 'completed',
                    current_step = 'completed',
                    progress_percentage = 100,
                    completed_at = CURRENT_TIMESTAMP,
                    updated_at = CURRENT_TIMESTAMP,
                    documents_created = :doc_types
                WHERE job_id = :job_id
            """
            session.execute(text(update_query), {
                "job_id": job_id,
                "doc_types": list(doc_types)
            })
            session.commit()
        
        logger.info("âœ… ETL processing completed successfully!")
        logger.info("ğŸ“Š Final Results:")
        logger.info(f"- Documents created: {len(documents)}")
        logger.info(f"- Document types: {len(doc_types)}")
        logger.info(f"- Document type distribution: {doc_type_counts}")
        
        # ì˜ˆìƒë˜ëŠ” 7ê°€ì§€ ë¬¸ì„œ íƒ€ì… í™•ì¸
        expected_types = {
            'USER_PROFILE', 'PERSONALITY_PROFILE', 'THINKING_SKILLS', 
            'CAREER_RECOMMENDATIONS', 'COMPETENCY_ANALYSIS', 'LEARNING_STYLE', 
            'PREFERENCE_ANALYSIS'
        }
        
        missing_types = expected_types - doc_types
        if missing_types:
            logger.warning(f"Missing document types: {missing_types}")
        else:
            logger.info("âœ… All 7 document types successfully created!")
        
        return {
            "status": "success",
            "job_id": job_id,
            "documents_created": len(documents),
            "document_types": len(doc_types),
            "document_type_distribution": doc_type_counts,
            "missing_types": list(missing_types)
        }
        
    except Exception as e:
        logger.error(f"âŒ ETL processing failed: {e}", exc_info=True)
        
        # ì‹¤íŒ¨ ìƒíƒœ ì—…ë°ì´íŠ¸
        with get_sync_session() as session:
            update_query = """
                UPDATE chat_etl_jobs 
                SET status = 'failed',
                    error_message = :error_message,
                    error_type = 'processing_error',
                    completed_at = CURRENT_TIMESTAMP,
                    updated_at = CURRENT_TIMESTAMP
                WHERE job_id = :job_id
            """
            session.execute(text(update_query), {
                "job_id": job_id,
                "error_message": str(e)
            })
            session.commit()
        
        raise
    finally:
        # Cleanup
        if 'executor' in locals():
            executor.cleanup()

if __name__ == "__main__":
    asyncio.run(trigger_improved_etl())