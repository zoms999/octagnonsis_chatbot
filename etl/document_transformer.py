"""
Document Transformation Engine
Converts query results into thematic documents optimized for RAG system with semantic chunking
"""

import logging
import time
from typing import Dict, Any, List, Optional
from dataclasses import dataclass
from datetime import datetime
import json
from collections import defaultdict

from database.models import DocumentType
from monitoring.preference_metrics import get_preference_metrics_collector

logger = logging.getLogger(__name__)

class DocumentTransformationError(Exception):
    """Raised when document transformation fails"""
    def __init__(self, doc_type: str, error_message: str):
        self.doc_type = doc_type
        self.error_message = error_message
        super().__init__(f"Document transformation failed for {doc_type}: {error_message}")

@dataclass
class TransformedDocument:
    """Container for transformed document data"""
    doc_type: str
    content: Dict[str, Any]
    summary_text: str
    metadata: Dict[str, Any]
    embedding_vector: Optional[List[float]] = None  # ì„ë² ë”© ë‹¨ê³„ì—ì„œ ì¶”ê°€ë¨

class DocumentTransformer:
    """
    Transforms raw query results into semantic documents optimized for RAG with chunking strategy
    """
    
    def __init__(self):
        # Remove the old transformation_methods approach for better chunking flexibility
        pass
    
    def _safe_get(self, data: List[Dict[str, Any]], index: int = 0, default: Dict[str, Any] = None) -> Dict[str, Any]:
        if default is None:
            default = {}
        if not data or len(data) <= index:
            return default
        return data[index] if data[index] is not None else default
    
    def _safe_get_value(self, data: Dict[str, Any], key: str, default: Any = None) -> Any:
        return data.get(key, default) if data else default
    
    def _generate_hypothetical_questions(self, summary: str, doc_type: str, content: Dict[str, Any]) -> List[str]:
        """
        ì£¼ì–´ì§„ ìš”ì•½ë¬¸ì„ ë°”íƒ•ìœ¼ë¡œ LLMì„ ì‚¬ìš©í•˜ì—¬ ê°€ìƒ ì§ˆë¬¸ì„ ìƒì„±í•©ë‹ˆë‹¤.
        ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” LLM API í˜¸ì¶œì´ í•„ìš”í•©ë‹ˆë‹¤.
        """
        # ----- ì‹¤ì œ êµ¬í˜„ ì‹œ LLM í˜¸ì¶œ ì˜ˆì‹œ -----
        # from your_llm_library import generate_text
        # prompt = f"""
        # ì•„ë˜ ë‚´ìš©ì€ ì‚¬ìš©ìì˜ ê²€ì‚¬ ê²°ê³¼ ë°ì´í„°ì˜ ì¼ë¶€ì…ë‹ˆë‹¤.
        # ì´ ë‚´ìš©ì„ ê°€ì¥ ì˜ ì„¤ëª…í•˜ê³  ìš”ì•½í•˜ëŠ” ì§ˆë¬¸ì„ í•œêµ­ì–´ë¡œ 3ê°œë§Œ ìƒì„±í•´ì£¼ì„¸ìš”.
        # ì§ˆë¬¸ì€ ì‚¬ìš©ìê°€ ì±—ë´‡ì—ê²Œ ë¬¼ì–´ë³¼ ë²•í•œ ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™”ì²´ì—¬ì•¼ í•©ë‹ˆë‹¤.
        #
        # ë‚´ìš©: "{summary}"
        #
        # ì§ˆë¬¸ (3ê°œ):
        # 1.
        # 2.
        # 3.
        # """
        # generated_text = generate_text(prompt)
        # questions = [line.strip() for line in generated_text.split('\n') if line.strip()]
        # return questions
        # -----------------------------------------
        
        # ì§€ê¸ˆì€ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ê·œì¹™ ê¸°ë°˜ìœ¼ë¡œ ì˜ˆì‹œ ì§ˆë¬¸ì„ ìƒì„±í•©ë‹ˆë‹¤.
        if "ê¸°ë³¸ ì •ë³´" in summary:
            return ["ë‚´ ë‚˜ì´ë‘ ì„±ë³„ ì•Œë ¤ì¤˜", "ë‚´ ê¸°ë³¸ ì •ë³´ ìš”ì•½í•´ì¤˜", "ë‚´ê°€ ëˆ„êµ¬ì¸ì§€ ì•Œë ¤ì¤˜"]
        if "í•™ë ¥" in summary:
            return ["ë‚´ ìµœì¢…í•™ë ¥ì€ ë­ì•¼?", "ë‚´ê°€ ì¡¸ì—…í•œ í•™êµë‘ ì „ê³µ ì•Œë ¤ì¤˜", "í•™ë ¥ ì •ë³´ ë³´ì—¬ì¤˜"]
        if "ì§ì—… ì •ë³´" in summary:
            return ["ë‚´ ì§ì—…ì´ ë­ì•¼?", "ì§€ê¸ˆ ë‹¤ë‹ˆëŠ” íšŒì‚¬ë‘ ì§ë¬´ ì•Œë ¤ì¤˜", "ê²½ë ¥ ì •ë³´ ìš”ì•½í•´ì¤˜"]
        if "ì£¼ìš” ì„±í–¥ ë¶„ì„" in summary:
            return ["ë‚´ ì„±ê²© ìœ í˜• ì•Œë ¤ì¤˜", "ë‚˜ì˜ ëŒ€í‘œì ì¸ ì„±í–¥ì€ ë­ì•¼?", "ì„±ê²© ê²€ì‚¬ ê²°ê³¼ ìš”ì•½í•´ì¤˜"]
        if "ì„±í–¥ì— ëŒ€í•œ ìƒì„¸ ì„¤ëª…" in summary:
            tendency_name = content.get("name", "ë‚´ ì„±í–¥")
            return [f"{tendency_name} ì„±í–¥ì€ ì–´ë–¤ íŠ¹ì§•ì´ ìˆì–´?", f"{tendency_name}ì— ëŒ€í•´ ìì„¸íˆ ì„¤ëª…í•´ì¤˜", f"ë‚´ ì„±ê²© ì§„ë‹¨ ê²°ê³¼ ì¢€ ë” ì•Œë ¤ì¤˜"]
        if "ì£¼ìš” ê°•ì " in summary:
            return ["ë‚´ ì„±ê²©ì˜ ê°•ì ì€ ë­ì•¼?", "ë‚´ê°€ ì˜í•˜ëŠ” ê±´ ë­ì•¼?", "ê°•ì  ë¶„ì„ ê²°ê³¼ ë³´ì—¬ì¤˜"]
        if "ê°œì„  ì˜ì—­" in summary:
            return ["ë‚´ ì„±ê²©ì˜ ì•½ì ì€ ë­ì•¼?", "ë‚´ê°€ ë³´ì™„í•´ì•¼ í•  ì ì€?", "ì•½ì  ë¶„ì„ ê²°ê³¼ ì•Œë ¤ì¤˜"]
        if "ì‚¬ê³ ë ¥: ë‚´ ì ìˆ˜" in summary:
            skill_name = content.get("skill_name", "ë‚´ ì‚¬ê³ ë ¥")
            return [f"ë‚´ {skill_name} ì ìˆ˜ëŠ” ëª‡ ì ì´ì•¼?", f"ë‚˜ëŠ” {skill_name}ì´ ê°•í•œ í¸ì´ì•¼?", f"{skill_name} ë¶„ì„ ê²°ê³¼ ì•Œë ¤ì¤˜"]
        if "ì„±í–¥ ê¸°ë°˜ ì¶”ì²œ ì§ì—…" in summary:
            return ["ë‚´ ì„±í–¥ì— ë§ëŠ” ì§ì—… ì¶”ì²œí•´ì¤˜", "ë‚˜í•œí…Œ ì–´ìš¸ë¦¬ëŠ” ì§ì—…ì´ ë­ì•¼?", "ì§„ë¡œ ì¶”ì²œ ê²°ê³¼ ì•Œë ¤ì¤˜"]
        if "ì—­ëŸ‰ ê¸°ë°˜ ì¶”ì²œ ì§ì—…" in summary:
            return ["ë‚´ ì—­ëŸ‰ìœ¼ë¡œ ê°ˆ ìˆ˜ ìˆëŠ” ì§ì—…ì€?", "ë‚´ ê°•ì ì„ ì‚´ë¦´ ìˆ˜ ìˆëŠ” ì§ì—… ì¶”ì²œí•´ì¤˜", "ì—­ëŸ‰ ê¸°ë°˜ ì§ì—… ì¶”ì²œ ê²°ê³¼ ë³´ì—¬ì¤˜"]
        
        # ë” êµ¬ì²´ì ì¸ íŒ¨í„´ ë§¤ì¹­ ì¶”ê°€
        if "ì„±í–¥" in summary or "ì„±ê²©" in summary:
            return ["ë‚´ì„±í–¥ì•Œë ¤ì¤˜", "ë‚´ ì„±ê²©ì€ ì–´ë–¤ íƒ€ì…ì´ì•¼?", "ì„±í–¥ ë¶„ì„ ê²°ê³¼ ë³´ì—¬ì¤˜"]
        if "ì‚¬ê³ ë ¥" in summary or "ì‚¬ê³ " in summary:
            return ["ë‚´ ì‚¬ê³ ë ¥ì€ ì–´ë•Œ?", "ì‚¬ê³  ëŠ¥ë ¥ ë¶„ì„ ê²°ê³¼ ì•Œë ¤ì¤˜", "ë‚´ê°€ ì–´ë–¤ ì‚¬ê³ ë¥¼ ì˜í•´?"]
        if "ì§ì—…" in summary or "ì§„ë¡œ" in summary:
            return ["ì¶”ì²œ ì§ì—… ì•Œë ¤ì¤˜", "ë‚˜í•œí…Œ ë§ëŠ” ì§ì—…ì´ ë­ì•¼?", "ì§„ë¡œ ì¶”ì²œí•´ì¤˜"]
        if "í•™ìŠµ" in summary:
            return ["ë‚´ í•™ìŠµ ìŠ¤íƒ€ì¼ì€?", "ì–´ë–»ê²Œ ê³µë¶€í•˜ëŠ” ê²Œ ì¢‹ì•„?", "í•™ìŠµ ë°©ë²• ì¶”ì²œí•´ì¤˜"]
        if "ì—­ëŸ‰" in summary or "ëŠ¥ë ¥" in summary:
            return ["ë‚´ ê°•ì ì€ ë­ì•¼?", "ë‚´ê°€ ì˜í•˜ëŠ” ëŠ¥ë ¥ì€?", "ì—­ëŸ‰ ë¶„ì„ ê²°ê³¼ ì•Œë ¤ì¤˜"]
        
        return [summary]  # ë§¤ì¹­ë˜ëŠ” ê·œì¹™ì´ ì—†ìœ¼ë©´ ê·¸ëƒ¥ ì›ë³¸ ìš”ì•½ë¬¸ì„ ì‚¬ìš©

    def _get_skill_level(self, percentile: float) -> str:
        """Determine skill level based on percentile"""
        if percentile >= 90: return "ë§¤ìš° ìš°ìˆ˜ (ìƒìœ„ 10%)"
        elif percentile >= 75: return "ìš°ìˆ˜ (ìƒìœ„ 25%)"
        elif percentile >= 50: return "ë³´í†µ (ìƒìœ„ 50%)"
        elif percentile >= 25: return "ê°œì„  í•„ìš”"
        else: return "ë§ì€ ê°œì„  í•„ìš”"

    # ==================== CHUNKING METHODS ====================
    # These methods create focused, topic-specific documents for better RAG performance
    
    def _generate_hypothetical_questions(self, summary: str, doc_type: str, content: Dict[str, Any]) -> List[str]:
        """ì£¼ì–´ì§„ ìš”ì•½ë¬¸ì„ ë°”íƒ•ìœ¼ë¡œ ê°€ìƒ ì§ˆë¬¸ì„ ìƒì„±í•©ë‹ˆë‹¤."""
        if "ê¸°ë³¸ ì •ë³´" in summary:
            return ["ë‚´ ê¸°ë³¸ ì •ë³´ ìš”ì•½í•´ì¤˜", "ë‚´ ë‚˜ì´ë‘ ì§ì—… ì•Œë ¤ì¤˜", "í”„ë¡œí•„ ì •ë³´ ë³´ì—¬ì¤˜"]
        if "í•™ë ¥" in summary:
            return ["ë‚´ í•™ë ¥ ì •ë³´ ì•Œë ¤ì¤˜", "ì–´ëŠ í•™êµ ë‹¤ë…”ì–´?", "ì „ê³µì´ ë­ì•¼?"]
        if "ì§ì—… ì •ë³´" in summary:
            return ["ë‚´ ì§ì—… ì •ë³´ ì•Œë ¤ì¤˜", "ì–´ë””ì„œ ì¼í•´?", "ë¬´ìŠ¨ ì¼ í•´?"]
        if "ì„±í–¥ ë¶„ì„" in summary:
            primary = content.get("primary_tendency", {}).get("name", "ë‚´ ì„±í–¥")
            return [f"ë‚´ ì„±ê²© ìœ í˜• ì•Œë ¤ì¤˜", f"ë‚˜ì˜ ì£¼ìš” ì„±í–¥ì€ ë­ì•¼?", f"{primary} ì„±í–¥ì— ëŒ€í•´ ì„¤ëª…í•´ì¤˜"]
        if "ì‚¬ê³ ë ¥" in summary:
            return ["ë‚´ ì‚¬ê³ ë ¥ ì ìˆ˜ ì•Œë ¤ì¤˜", "ë‚˜ëŠ” ì–´ë–¤ ì‚¬ê³ ë¥¼ ì˜í•´?", "ì‚¬ê³ ë ¥ ë¶„ì„ ê²°ê³¼ ìš”ì•½í•´ì¤˜"]
        if "ì¶”ì²œ ì§ì—…" in summary or "ì§ì—…" in summary:
            return ["ë‚˜í•œí…Œ ë§ëŠ” ì§ì—… ì¶”ì²œí•´ì¤˜", "ë‚´ ì„±í–¥ì— ì–´ìš¸ë¦¬ëŠ” ì§ì—…ì€?", "ì§„ë¡œ ì¶”ì²œ ê²°ê³¼ ì•Œë ¤ì¤˜"]
        if "í•™ìŠµ ìŠ¤íƒ€ì¼" in summary:
            return ["ë‚˜í•œí…Œ ë§ëŠ” ê³µë¶€ ë°©ë²• ì•Œë ¤ì¤˜", "ë‚´ í•™ìŠµ ìŠ¤íƒ€ì¼ì€ ì–´ë•Œ?", "ì–´ë–»ê²Œ ê³µë¶€í•´ì•¼ íš¨ìœ¨ì ì¼ê¹Œ?"]
        if "í•µì‹¬ ì—­ëŸ‰" in summary or "ì—­ëŸ‰" in summary:
            return ["ë‚´ê°€ ê°€ì§„ í•µì‹¬ ì—­ëŸ‰ì€ ë­ì•¼?", "ë‚˜ì˜ ê°•ì  ì—­ëŸ‰ ì•Œë ¤ì¤˜", "ì—­ëŸ‰ ë¶„ì„ ê²°ê³¼ ë³´ì—¬ì¤˜"]
        if "ì„ í˜¸ë„" in summary or "ì´ë¯¸ì§€" in summary:
            return ["ë‚´ ì„ í˜¸ë„ ë¶„ì„ ê²°ê³¼ ì•Œë ¤ì¤˜", "ì´ë¯¸ì§€ ì„ í˜¸ë„ ê²€ì‚¬ ê²°ê³¼ëŠ”?", "ë‚´ê°€ ì„ í˜¸í•˜ëŠ” ê²ƒë“¤ì€ ë­ì•¼?"]
        # ê¸°ë³¸ ì§ˆë¬¸
        return [f"{summary}ì— ëŒ€í•´ ì•Œë ¤ì¤˜", "ê²°ê³¼ë¥¼ ìì„¸íˆ ì„¤ëª…í•´ì¤˜"]
    
    def _chunk_user_profile(self, query_results: Dict[str, List[Dict[str, Any]]]) -> List[TransformedDocument]:
        """Create focused user profile documents"""
        documents = []
        personal_info = self._safe_get(query_results.get("personalInfoQuery", []))
        institute_settings = self._safe_get(query_results.get("instituteSettingsQuery", []))
        
        if not personal_info or 'user_name' not in personal_info:
            # ê°œì¸ì •ë³´ ë°ì´í„°ê°€ ì—†ì„ ë•Œ ê¸°ë³¸ ë¬¸ì„œ ìƒì„±
            logger.warning("ê°œì¸ì •ë³´ ë°ì´í„°ê°€ ì—†ì–´ ê¸°ë³¸ ë¬¸ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
            documents.append(TransformedDocument(
                doc_type="USER_PROFILE",
                content={"message": "ì‚¬ìš©ì í”„ë¡œí•„ ë°ì´í„°ê°€ ì•„ì§ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."},
                summary_text="ì‚¬ìš©ì í”„ë¡œí•„: ë°ì´í„° ì¤€ë¹„ ì¤‘",
                metadata={"data_sources": [], "created_at": datetime.now().isoformat(), "sub_type": "unavailable"}
            ))
            return documents

        user_name = self._safe_get_value(personal_info, "user_name", "ì‚¬ìš©ì")

        # 1. Basic Profile Document
        basic_content = {
            "user_name": user_name,
            "age": self._safe_get_value(personal_info, "age"),
            "gender": self._safe_get_value(personal_info, "gender"),
            "birth_date": self._safe_get_value(personal_info, "birth_date")
        }
        
        summary = f"{user_name}ë‹˜ì˜ ê¸°ë³¸ ì •ë³´: {basic_content['age']}ì„¸, {basic_content['gender']}"
        documents.append(TransformedDocument(
            doc_type="USER_PROFILE",
            content=basic_content,
            summary_text=summary,
            metadata={"data_sources": ["personalInfoQuery"], "created_at": datetime.now().isoformat(), "sub_type": "basic_info"}
        ))

        # 2. Education Document
        education_info = {
            "education_level": self._safe_get_value(personal_info, "education_level"),
            "school_name": self._safe_get_value(personal_info, "school_name"),
            "school_year": self._safe_get_value(personal_info, "school_year"),
            "major": self._safe_get_value(personal_info, "major")
        }
        
        if education_info.get("school_name") or education_info.get("education_level"):
            edu_summary = f"{user_name}ë‹˜ì˜ í•™ë ¥: {education_info['education_level']}"
            if education_info.get("school_name"):
                edu_summary += f", {education_info['school_name']}"
            if education_info.get("major"):
                edu_summary += f"ì—ì„œ {education_info['major']} ì „ê³µ"
                
            documents.append(TransformedDocument(
                doc_type="USER_PROFILE",
                content=education_info,
                summary_text=edu_summary,
                metadata={"data_sources": ["personalInfoQuery"], "created_at": datetime.now().isoformat(), "sub_type": "education"}
            ))

        # 3. Career Document
        career_info = {
            "job_status": self._safe_get_value(personal_info, "job_status"),
            "company_name": self._safe_get_value(personal_info, "company_name"),
            "job_title": self._safe_get_value(personal_info, "job_title")
        }
        
        if career_info.get("job_status") or career_info.get("company_name"):
            career_summary = f"{user_name}ë‹˜ì˜ ì§ì—… ì •ë³´: {career_info['job_status']}"
            if career_info.get("company_name"):
                career_summary += f", {career_info['company_name']}"
            if career_info.get("job_title"):
                career_summary += f"ì—ì„œ {career_info['job_title']} ë‹´ë‹¹"
                
            documents.append(TransformedDocument(
                doc_type="USER_PROFILE",
                content=career_info,
                summary_text=career_summary,
                metadata={"data_sources": ["personalInfoQuery"], "created_at": datetime.now().isoformat(), "sub_type": "career"}
            ))

        return documents

    def _chunk_personality_analysis(self, query_results: Dict[str, List[Dict[str, Any]]]) -> List[TransformedDocument]:
        """Create detailed personality analysis documents"""
        documents = []
        
        # Main tendency summary
        tendency_data = self._safe_get(query_results.get("tendencyQuery", []))
        top_tendencies = query_results.get("topTendencyQuery", [])
        tendency_stats = query_results.get("tendencyStatsQuery", [])
        
        if tendency_data:
            primary = self._safe_get_value(tendency_data, "Tnd1")
            secondary = self._safe_get_value(tendency_data, "Tnd2")
            tertiary = self._safe_get_value(tendency_data, "Tnd3")
            
            # Find stats for each tendency
            primary_stats = next((s for s in tendency_stats if primary and s.get('tendency_name', '').startswith(primary)), {})
            secondary_stats = next((s for s in tendency_stats if secondary and s.get('tendency_name', '').startswith(secondary)), {})
            tertiary_stats = next((s for s in tendency_stats if tertiary and s.get('tendency_name', '').startswith(tertiary)), {})
            
            # í†µê³„ ë°ì´í„°ì—ì„œ ê° ì„±í–¥ì˜ ë¹„ìœ¨ ì°¾ê¸°
            stats_map = {}
            for stat in tendency_stats:
                tendency_name = stat.get('tendency_name', '').replace('í˜•', '')
                stats_map[tendency_name] = stat.get('percentage', 0)
            
            content = {
                "primary_tendency": {"name": primary, "percentage": stats_map.get(primary, 0)},
                "secondary_tendency": {"name": secondary, "percentage": stats_map.get(secondary, 0)},
                "tertiary_tendency": {"name": tertiary, "percentage": stats_map.get(tertiary, 0)}
            }
            
            summary = f"ì£¼ìš” ì„±í–¥ ë¶„ì„: 1ìˆœìœ„ {primary}({content['primary_tendency']['percentage']:.1f}%), 2ìˆœìœ„ {secondary}({content['secondary_tendency']['percentage']:.1f}%)"
            if tertiary:
                summary += f", 3ìˆœìœ„ {tertiary}({content['tertiary_tendency']['percentage']:.1f}%)"
                
            documents.append(TransformedDocument(
                doc_type="PERSONALITY_PROFILE",
                content=content,
                summary_text=summary,
                metadata={"data_sources": ["tendencyQuery", "tendencyStatsQuery"], "created_at": datetime.now().isoformat(), "sub_type": "main_tendencies"}
            ))
        else:
            # ì„±í–¥ ë°ì´í„°ê°€ ì—†ì„ ë•ŒëŠ” ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜ (1ë‹¨ê³„ì—ì„œ ë°ì´í„° ì¤€ë¹„ë¥¼ ë³´ì¥í•˜ë¯€ë¡œ)
            logger.warning("ì„±í–¥ ë¶„ì„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return []

        # Individual tendency explanations
        tendency1_explain = self._safe_get(query_results.get("tendency1ExplainQuery", []))
        if tendency1_explain and tendency1_explain.get("explanation"):
            primary_name = self._safe_get_value(tendency_data, "Tnd1", "1ìˆœìœ„ ì„±í–¥")
            documents.append(TransformedDocument(
                doc_type="PERSONALITY_PROFILE",
                content=tendency1_explain,
                summary_text=f"{primary_name} ì„±í–¥ì— ëŒ€í•œ ìƒì„¸ ì„¤ëª…: {tendency1_explain['explanation'][:100]}...",
                metadata={"data_sources": ["tendency1ExplainQuery"], "created_at": datetime.now().isoformat(), "sub_type": "tendency_1_explanation"}
            ))

        tendency2_explain = self._safe_get(query_results.get("tendency2ExplainQuery", []))
        if tendency2_explain and tendency2_explain.get("explanation"):
            secondary_name = self._safe_get_value(tendency_data, "Tnd2", "2ìˆœìœ„ ì„±í–¥")
            documents.append(TransformedDocument(
                doc_type="PERSONALITY_PROFILE",
                content=tendency2_explain,
                summary_text=f"{secondary_name} ì„±í–¥ì— ëŒ€í•œ ìƒì„¸ ì„¤ëª…: {tendency2_explain['explanation'][:100]}...",
                metadata={"data_sources": ["tendency2ExplainQuery"], "created_at": datetime.now().isoformat(), "sub_type": "tendency_2_explanation"}
            ))

        # Top tendencies with detailed explanations
        top_tendency_explains = query_results.get("topTendencyExplainQuery", [])
        for i, explain_data in enumerate(top_tendency_explains[:5]):  # Top 5 only
            if explain_data.get("explanation"):
                documents.append(TransformedDocument(
                    doc_type="PERSONALITY_PROFILE",
                    content=explain_data,
                    summary_text=f"{explain_data.get('tendency_name', f'{i+1}ìˆœìœ„ ì„±í–¥')} ìƒì„¸ ë¶„ì„: {explain_data['explanation'][:100]}...",
                    metadata={"data_sources": ["topTendencyExplainQuery"], "created_at": datetime.now().isoformat(), "sub_type": f"top_tendency_detail_{i+1}"}
                ))

        # Top tendencies with detailed explanations
        top_tendency_explains = query_results.get("topTendencyExplainQuery", [])
        for i, explain_data in enumerate(top_tendency_explains[:5]):  # Top 5 only
            if explain_data.get("explanation"):
                documents.append(TransformedDocument(
                    doc_type="PERSONALITY_PROFILE",
                    content=explain_data,
                    summary_text=f"{explain_data.get('tendency_name', f'{i+1}ìˆœìœ„ ì„±í–¥')} ìƒì„¸ ë¶„ì„: {explain_data['explanation'][:100]}...",
                    metadata={"data_sources": ["topTendencyExplainQuery"], "created_at": datetime.now().isoformat(), "sub_type": f"top_tendency_detail_{i+1}"}
                ))

        # Strengths and weaknesses
        strengths_weaknesses = query_results.get("strengthsWeaknessesQuery", [])
        if strengths_weaknesses:
            strengths = [sw for sw in strengths_weaknesses if sw.get('type') == 'strength']
            weaknesses = [sw for sw in strengths_weaknesses if sw.get('type') == 'weakness']
            
            if strengths:
                strength_summary = f"ì£¼ìš” ê°•ì : {', '.join([s['description'][:50] for s in strengths[:3]])}"
                documents.append(TransformedDocument(
                    doc_type="PERSONALITY_PROFILE",
                    content={"strengths": strengths},
                    summary_text=strength_summary,
                    metadata={"data_sources": ["strengthsWeaknessesQuery"], "created_at": datetime.now().isoformat(), "sub_type": "strengths"}
                ))
            
            if weaknesses:
                weakness_summary = f"ê°œì„  ì˜ì—­: {', '.join([w['description'][:50] for w in weaknesses[:3]])}"
                documents.append(TransformedDocument(
                    doc_type="PERSONALITY_PROFILE",
                    content={"weaknesses": weaknesses},
                    summary_text=weakness_summary,
                    metadata={"data_sources": ["strengthsWeaknessesQuery"], "created_at": datetime.now().isoformat(), "sub_type": "weaknesses"}
                ))

        return documents

    def _chunk_thinking_skills(self, query_results: Dict[str, List[Dict[str, Any]]]) -> List[TransformedDocument]:
        """Create focused thinking skills documents"""
        documents = []
        
        # Main thinking skills summary
        thinking_main = self._safe_get(query_results.get("thinkingMainQuery", []))
        thinking_skills = query_results.get("thinkingSkillsQuery", [])
        
        if thinking_main:
            summary = f"ì£¼ìš” ì‚¬ê³ ë ¥: {thinking_main.get('main_thinking_skill')}, ë¶€ ì‚¬ê³ ë ¥: {thinking_main.get('sub_thinking_skill')}, ì´ì : {thinking_main.get('total_score')}"
            documents.append(TransformedDocument(
                doc_type="THINKING_SKILLS",
                content=thinking_main,
                summary_text=summary,
                metadata={"data_sources": ["thinkingMainQuery"], "created_at": datetime.now().isoformat(), "sub_type": "summary"}
            ))
        elif thinking_skills:
            # thinkingSkillsQuery ë°ì´í„°ë¡œ ìš”ì•½ ìƒì„±
            skill_names = [skill.get('skill_name', '') for skill in thinking_skills[:3]]
            summary = f"ì‚¬ê³ ë ¥ ë¶„ì„: {', '.join(skill_names)} ë“± {len(thinking_skills)}ê°œ ì˜ì—­"
            documents.append(TransformedDocument(
                doc_type="THINKING_SKILLS",
                content={"skills": thinking_skills},
                summary_text=summary,
                metadata={"data_sources": ["thinkingSkillsQuery"], "created_at": datetime.now().isoformat(), "sub_type": "skills_overview"}
            ))

        # Detailed thinking skills comparison
        comparison_data = query_results.get("thinkingSkillComparisonQuery", [])
        if comparison_data:
            # Create individual documents for top skills
            sorted_skills = sorted(comparison_data, key=lambda x: x.get('my_score', 0), reverse=True)
            
            for i, skill in enumerate(sorted_skills[:5]):  # Top 5 skills
                skill_name = skill.get('skill_name')
                my_score = skill.get('my_score', 0)
                avg_score = skill.get('average_score', 0)
                
                summary = f"{skill_name} ì‚¬ê³ ë ¥: ë‚´ ì ìˆ˜ {my_score}ì , í‰ê·  {avg_score}ì "
                if my_score > avg_score:
                    summary += f" (í‰ê· ë³´ë‹¤ {my_score - avg_score}ì  ë†’ìŒ)"
                elif my_score < avg_score:
                    summary += f" (í‰ê· ë³´ë‹¤ {avg_score - my_score}ì  ë‚®ìŒ)"
                
                documents.append(TransformedDocument(
                    doc_type="THINKING_SKILLS",
                    content=skill,
                    summary_text=summary,
                    metadata={"data_sources": ["thinkingSkillComparisonQuery"], "created_at": datetime.now().isoformat(), "sub_type": f"skill_{i+1}", "skill_name": skill_name}
                ))

        # Detailed thinking explanations
        thinking_details = query_results.get("thinkingDetailQuery", [])
        for detail in thinking_details:
            if detail.get("explanation"):
                skill_name = detail.get('skill_name')
                documents.append(TransformedDocument(
                    doc_type="THINKING_SKILLS",
                    content=detail,
                    summary_text=f"{skill_name} ìƒì„¸ ë¶„ì„: {detail['explanation'][:100]}...",
                    metadata={"data_sources": ["thinkingDetailQuery"], "created_at": datetime.now().isoformat(), "sub_type": "detail", "skill_name": skill_name}
                ))

        # Detailed thinking explanations
        thinking_details = query_results.get("thinkingDetailQuery", [])
        for detail in thinking_details:
            if detail.get("explanation"):
                skill_name = detail.get('skill_name')
                documents.append(TransformedDocument(
                    doc_type="THINKING_SKILLS",
                    content=detail,
                    summary_text=f"{skill_name} ìƒì„¸ ë¶„ì„: {detail['explanation'][:100]}...",
                    metadata={"data_sources": ["thinkingDetailQuery"], "created_at": datetime.now().isoformat(), "sub_type": "detail", "skill_name": skill_name}
                ))

        return documents

    def _chunk_career_recommendations(self, query_results: Dict[str, List[Dict[str, Any]]]) -> List[TransformedDocument]:
        """Create separate documents for different types of career recommendations"""
        documents = []

        # Tendency-based job recommendations
        tendency_jobs = query_results.get("careerRecommendationQuery", [])
        if tendency_jobs:
            job_names = [job['job_name'] for job in tendency_jobs[:5]]
            summary = f"ì„±í–¥ ê¸°ë°˜ ì¶”ì²œ ì§ì—…: {', '.join(job_names)}"
            documents.append(TransformedDocument(
                doc_type="CAREER_RECOMMENDATIONS",
                content={"jobs": tendency_jobs, "recommendation_type": "tendency"},
                summary_text=summary,
                metadata={"data_sources": ["careerRecommendationQuery"], "created_at": datetime.now().isoformat(), "sub_type": "tendency_based"}
            ))

        # Competency-based job recommendations
        competency_jobs = query_results.get("competencyJobsQuery", [])
        if competency_jobs:
            job_names = [job['jo_name'] for job in competency_jobs[:5]]
            summary = f"ì—­ëŸ‰ ê¸°ë°˜ ì¶”ì²œ ì§ì—…: {', '.join(job_names)}"
            documents.append(TransformedDocument(
                doc_type="CAREER_RECOMMENDATIONS",
                content={"jobs": competency_jobs, "recommendation_type": "competency"},
                summary_text=summary,
                metadata={"data_sources": ["competencyJobsQuery"], "created_at": datetime.now().isoformat(), "sub_type": "competency_based"}
            ))

        # Preference-based job recommendations
        preference_jobs = query_results.get("preferenceJobsQuery", [])
        if preference_jobs:
            # Group by preference type
            pref_groups = defaultdict(list)
            for job in preference_jobs:
                pref_groups[job.get('preference_type', 'unknown')].append(job)
            
            for pref_type, jobs in pref_groups.items():
                job_names = [job['jo_name'] for job in jobs[:3]]
                pref_name = jobs[0].get('preference_name', pref_type)
                summary = f"{pref_name} ì„ í˜¸ë„ ê¸°ë°˜ ì¶”ì²œ ì§ì—…: {', '.join(job_names)}"
                documents.append(TransformedDocument(
                    doc_type="CAREER_RECOMMENDATIONS",
                    content={"jobs": jobs, "preference_type": pref_type, "preference_name": pref_name},
                    summary_text=summary,
                    metadata={"data_sources": ["preferenceJobsQuery"], "created_at": datetime.now().isoformat(), "sub_type": f"preference_{pref_type}"}
                ))

        # Job majors recommendations
        job_majors = query_results.get("suitableJobMajorsQuery", [])
        if job_majors:
            summary = f"ì¶”ì²œ ì§ì—…ë³„ ê´€ë ¨ ì „ê³µ: {', '.join([jm['jo_name'] for jm in job_majors[:3]])}"
            documents.append(TransformedDocument(
                doc_type="CAREER_RECOMMENDATIONS",
                content={"job_majors": job_majors},
                summary_text=summary,
                metadata={"data_sources": ["suitableJobMajorsQuery"], "created_at": datetime.now().isoformat(), "sub_type": "related_majors"}
            ))

        # Duties recommendations
        duties = query_results.get("dutiesQuery", [])
        if duties:
            duty_names = [duty['du_name'] for duty in duties[:5]]
            summary = f"ì¶”ì²œ ì§ë¬´: {', '.join(duty_names)}"
            documents.append(TransformedDocument(
                doc_type="CAREER_RECOMMENDATIONS",
                content={"duties": duties},
                summary_text=summary,
                metadata={"data_sources": ["dutiesQuery"], "created_at": datetime.now().isoformat(), "sub_type": "duties"}
            ))

        return documents

    def _chunk_competency_analysis(self, query_results: Dict[str, List[Dict[str, Any]]]) -> List[TransformedDocument]:
        """Create detailed competency analysis documents"""
        documents = []
        
        competencies = query_results.get("competencyAnalysisQuery", [])
        competency_subjects = query_results.get("competencySubjectsQuery", [])
        talent_list = self._safe_get(query_results.get("talentListQuery", []))

        # Overall competency summary
        if talent_list and talent_list.get("talent_summary"):
            documents.append(TransformedDocument(
                doc_type="COMPETENCY_ANALYSIS",
                content=talent_list,
                summary_text=f"í•µì‹¬ ì—­ëŸ‰ ìš”ì•½: {talent_list['talent_summary']}",
                metadata={"data_sources": ["talentListQuery"], "created_at": datetime.now().isoformat(), "sub_type": "summary"}
            ))
        elif competencies:
            # talentListQueryê°€ ì—†ì–´ë„ competencyAnalysisQueryê°€ ìˆìœ¼ë©´ ìš”ì•½ ìƒì„±
            comp_names = [comp.get('competency_name', '') for comp in competencies[:5]]
            summary_text = f"í•µì‹¬ ì—­ëŸ‰ ìš”ì•½: {', '.join(comp_names)}"
            documents.append(TransformedDocument(
                doc_type="COMPETENCY_ANALYSIS",
                content={"competencies": competencies},
                summary_text=summary_text,
                metadata={"data_sources": ["competencyAnalysisQuery"], "created_at": datetime.now().isoformat(), "sub_type": "summary"}
            ))
        else:
            # ì—­ëŸ‰ ë°ì´í„°ê°€ ì—†ì„ ë•ŒëŠ” ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
            logger.warning("ì—­ëŸ‰ ë¶„ì„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return []

        # Individual competency details
        subjects_by_competency = defaultdict(list)
        for sub in competency_subjects:
            subjects_by_competency[sub.get('competency_name', '')].append(sub)

        for comp in competencies:
            comp_name = comp.get('competency_name')
            if comp_name:
                related_subjects = subjects_by_competency.get(comp_name, [])
                content = {
                    "competency": comp,
                    "related_subjects": related_subjects
                }
                
                summary = f"{comp_name} ì—­ëŸ‰: {comp.get('score')}ì  (ìƒìœ„ {comp.get('percentile')}%)"
                if related_subjects:
                    subject_names = [s['subject_name'] for s in related_subjects[:3]]
                    summary += f", ê´€ë ¨ ê³¼ëª©: {', '.join(subject_names)}"
                
                documents.append(TransformedDocument(
                    doc_type="COMPETENCY_ANALYSIS",
                    content=content,
                    summary_text=summary,
                    metadata={"data_sources": ["competencyAnalysisQuery", "competencySubjectsQuery"], "created_at": datetime.now().isoformat(), "sub_type": f"competency_{comp.get('rank', 0)}", "competency_name": comp_name}
                ))

        return documents

    def _chunk_learning_style(self, query_results: Dict[str, List[Dict[str, Any]]]) -> List[TransformedDocument]:
        """Create learning style documents"""
        documents = []
        
        learning_style = self._safe_get(query_results.get("learningStyleQuery", []))
        learning_chart = query_results.get("learningStyleChartQuery", [])
        subject_ranks = query_results.get("subjectRanksQuery", [])

        if learning_style:
            # Main learning style document
            summary = f"í•™ìŠµ ìŠ¤íƒ€ì¼: {learning_style.get('tnd1_name')} ê¸°ë°˜"
            if learning_style.get('tnd1_study_tendency'):
                summary += f", í•™ìŠµ ì„±í–¥: {learning_style['tnd1_study_tendency'][:50]}..."
            
            documents.append(TransformedDocument(
                doc_type="LEARNING_STYLE",
                content=learning_style,
                summary_text=summary,
                metadata={"data_sources": ["learningStyleQuery"], "created_at": datetime.now().isoformat(), "sub_type": "main"}
            ))

        # Subject recommendations
        if subject_ranks:
            top_subjects = subject_ranks[:5]
            subject_names = [s['subject_name'] for s in top_subjects]
            summary = f"ì¶”ì²œ í•™ìŠµ ê³¼ëª©: {', '.join(subject_names)}"
            
            documents.append(TransformedDocument(
                doc_type="LEARNING_STYLE",
                content={"subjects": top_subjects},
                summary_text=summary,
                metadata={"data_sources": ["subjectRanksQuery"], "created_at": datetime.now().isoformat(), "sub_type": "recommended_subjects"}
            ))

        # Learning method chart data
        if learning_chart:
            style_data = [item for item in learning_chart if item.get('item_type') == 'S']
            method_data = [item for item in learning_chart if item.get('item_type') == 'W']
            
            if style_data:
                documents.append(TransformedDocument(
                    doc_type="LEARNING_STYLE",
                    content={"style_data": style_data},
                    summary_text=f"í•™ìŠµ ìŠ¤íƒ€ì¼ ë¶„ì„: {', '.join([s['item_name'] for s in style_data[:3]])}",
                    metadata={"data_sources": ["learningStyleChartQuery"], "created_at": datetime.now().isoformat(), "sub_type": "style_chart"}
                ))
            
            if method_data:
                documents.append(TransformedDocument(
                    doc_type="LEARNING_STYLE",
                    content={"method_data": method_data},
                    summary_text=f"í•™ìŠµ ë°©ë²• ë¶„ì„: {', '.join([m['item_name'] for m in method_data[:3]])}",
                    metadata={"data_sources": ["learningStyleChartQuery"], "created_at": datetime.now().isoformat(), "sub_type": "method_chart"}
                ))

        # í•™ìŠµ ìŠ¤íƒ€ì¼ ë°ì´í„°ê°€ ì „í˜€ ì—†ì„ ë•Œ ê¸°ë³¸ ë¬¸ì„œ ìƒì„±
        if not documents:
            logger.warning("í•™ìŠµ ìŠ¤íƒ€ì¼ ë°ì´í„°ê°€ ì—†ì–´ ê¸°ë³¸ ë¬¸ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
            documents.append(TransformedDocument(
                doc_type="LEARNING_STYLE",
                content={"message": "í•™ìŠµ ìŠ¤íƒ€ì¼ ë¶„ì„ ë°ì´í„°ê°€ ì•„ì§ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."},
                summary_text="í•™ìŠµ ìŠ¤íƒ€ì¼: ë°ì´í„° ì¤€ë¹„ ì¤‘",
                metadata={"data_sources": [], "created_at": datetime.now().isoformat(), "sub_type": "unavailable"}
            ))

        return documents

    def _chunk_preference_analysis(self, query_results: Dict[str, List[Dict[str, Any]]]) -> List[TransformedDocument]:
        """Create enhanced preference analysis documents with intelligent fallback handling"""
        start_time = time.time()
        documents = []
        documents_created = 0
        documents_failed = 0
        
        # Get metrics collector for monitoring
        metrics_collector = get_preference_metrics_collector()
        
        # Extract all preference-related data
        preference_stats = self._safe_get(query_results.get("imagePreferenceStatsQuery", []))
        preference_data = query_results.get("preferenceDataQuery", [])
        preference_jobs = query_results.get("preferenceJobsQuery", [])
        
        # Track what data is available for intelligent fallback
        available_data = {
            "stats": bool(preference_stats and preference_stats.get('total_image_count')),
            "preferences": bool(preference_data),
            "jobs": bool(preference_jobs)
        }
        
        # Count available data components
        available_count = sum(available_data.values())
        
        # Calculate data completeness score
        data_completeness_score = available_count / 3.0
        
        # Create documents based on data availability
        try:
            if available_count == 0:
                # No preference data available - create comprehensive fallback document
                documents.append(self._create_preference_fallback_document(available_data))
                documents_created = 1
            elif available_count < 3:
                # Partial data available - create partial document + available data documents
                partial_content = {
                    "stats": preference_stats if available_data["stats"] else None,
                    "preferences": preference_data if available_data["preferences"] else None,
                    "jobs": preference_jobs if available_data["jobs"] else None
                }
                documents.append(self._create_partial_preference_document(available_data, partial_content))
                documents_created += 1
                
                # Create documents for available data
                if available_data["stats"]:
                    stats_docs = self._create_preference_stats_document(preference_stats, available_data)
                    documents.extend(stats_docs)
                    documents_created += len(stats_docs)
                if available_data["preferences"]:
                    pref_docs = self._create_preference_data_documents(preference_data, available_data)
                    documents.extend(pref_docs)
                    documents_created += len(pref_docs)
                if available_data["jobs"]:
                    job_docs = self._create_preference_jobs_documents(preference_jobs, available_data)
                    documents.extend(job_docs)
                    documents_created += len(job_docs)
            else:
                # All data available - create complete documents
                stats_docs = self._create_preference_stats_document(preference_stats, available_data)
                documents.extend(stats_docs)
                documents_created += len(stats_docs)
                
                pref_docs = self._create_preference_data_documents(preference_data, available_data)
                documents.extend(pref_docs)
                documents_created += len(pref_docs)
                
                job_docs = self._create_preference_jobs_documents(preference_jobs, available_data)
                documents.extend(job_docs)
                documents_created += len(job_docs)
                
                # Add completion summary document
                documents.append(self._create_preference_completion_summary(preference_stats, preference_data, preference_jobs))
                documents_created += 1
                
            success = True
            error_message = None
            
        except Exception as e:
            logger.error(f"Error creating preference documents: {e}")
            documents_failed = 1
            success = False
            error_message = str(e)
            
            # Create fallback error document
            documents.append(self._create_preference_error_document(str(e)))
            documents_created = 1
        
        # Record document creation metrics
        processing_time_ms = (time.time() - start_time) * 1000
        
        # Extract anp_seq from query results context if available
        anp_seq = getattr(self, '_current_anp_seq', 0)  # This would need to be set by the caller
        
        # Record metrics asynchronously
        import asyncio
        try:
            asyncio.create_task(metrics_collector.record_document_creation(
                anp_seq=anp_seq,
                documents_created=documents_created,
                documents_failed=documents_failed,
                total_processing_time_ms=processing_time_ms,
                data_completeness_score=data_completeness_score,
                success=success,
                error_message=error_message
            ))
        except Exception as e:
            logger.warning(f"Failed to record preference document metrics: {e}")
        
        return documents

    def _create_preference_error_document(self, error_message: str) -> TransformedDocument:
        """Create error document when preference processing fails"""
        content = {
            "error": True,
            "message": "ì„ í˜¸ë„ ë¶„ì„ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
            "technical_details": error_message,
            "recommendations": [
                "ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”.",
                "ë¬¸ì œê°€ ì§€ì†ë˜ë©´ ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”."
            ]
        }
        
        summary_text = "ì„ í˜¸ë„ ë¶„ì„ ì²˜ë¦¬ ì˜¤ë¥˜ - ê¸°ìˆ ì  ë¬¸ì œë¡œ ì¸í•´ ì„ í˜¸ë„ ë¶„ì„ì„ ì™„ë£Œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        return TransformedDocument(
            doc_type=DocumentType.PREFERENCE_ANALYSIS.value,
            content=content,
            summary_text=summary_text,
            metadata={
                "error": True,
                "timestamp": datetime.now().isoformat(),
                "processing_status": "failed"
            }
        )

    def _create_preference_completion_summary(self, stats: Dict[str, Any], preferences: List[Dict[str, Any]], jobs: List[Dict[str, Any]]) -> TransformedDocument:
        """Create summary document when all preference data is available"""
        
        # Extract key metrics
        response_rate = stats.get('response_rate', 0) if stats else 0
        pref_count = len(preferences) if preferences else 0
        job_count = len(jobs) if jobs else 0
        
        # Create comprehensive summary
        summary_text = f"ì„ í˜¸ë„ ë¶„ì„ ì™„ë£Œ: {pref_count}ê°œ ì„ í˜¸ ì˜ì—­, {job_count}ê°œ ì¶”ì²œ ì§ì—…"
        if response_rate:
            summary_text += f" (ê²€ì‚¬ ì‘ë‹µë¥  {response_rate}%)"
        
        # Generate insights
        insights = []
        if response_rate >= 80:
            insights.append("ê²€ì‚¬ê°€ ì¶©ë¶„íˆ ì™„ë£Œë˜ì–´ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ë¶„ì„ ê²°ê³¼ì…ë‹ˆë‹¤.")
        if pref_count >= 5:
            insights.append("ë‹¤ì–‘í•œ ì„ í˜¸ ì˜ì—­ì´ ì‹ë³„ë˜ì–´ í­ë„“ì€ ê´€ì‹¬ì‚¬ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.")
        if job_count >= 10:
            insights.append("ë§ì€ ì§ì—… ì˜µì…˜ì´ ì œì‹œë˜ì–´ ì„ íƒì˜ í­ì´ ë„“ìŠµë‹ˆë‹¤.")
        
        # Get top preferences
        top_preferences = []
        if preferences:
            sorted_prefs = sorted(preferences, key=lambda x: x.get('rank', 999))[:3]
            top_preferences = [p.get('preference_name', '') for p in sorted_prefs if p.get('preference_name')]
        
        content = {
            "completion_status": "ì™„ë£Œ",
            "response_rate": response_rate,
            "preference_count": pref_count,
            "job_count": job_count,
            "top_preferences": top_preferences,
            "insights": insights,
            "quality_score": self._calculate_preference_quality_score(response_rate, pref_count, job_count),
            "recommendation": "ëª¨ë“  ì„ í˜¸ë„ ë¶„ì„ ê²°ê³¼ë¥¼ ì¢…í•©ì ìœ¼ë¡œ ê²€í† í•˜ì—¬ ì§„ë¡œ ë°©í–¥ì„ ì„¤ì •í•´ë³´ì„¸ìš”."
        }
        
        return TransformedDocument(
            doc_type="PREFERENCE_ANALYSIS",
            content=content,
            summary_text=summary_text,
            metadata={
                "data_sources": ["imagePreferenceStatsQuery", "preferenceDataQuery", "preferenceJobsQuery"], 
                "created_at": datetime.now().isoformat(), 
                "sub_type": "completion_summary",
                "completion_level": "complete",
                "quality_score": content["quality_score"]
            }
        )

    def _calculate_preference_quality_score(self, response_rate: float, pref_count: int, job_count: int) -> float:
        """Calculate quality score for preference analysis completeness"""
        score = 0.0
        
        # Response rate component (40% of score)
        if response_rate >= 90:
            score += 40
        elif response_rate >= 80:
            score += 35
        elif response_rate >= 70:
            score += 30
        elif response_rate >= 50:
            score += 20
        else:
            score += 10
        
        # Preference count component (30% of score)
        if pref_count >= 8:
            score += 30
        elif pref_count >= 5:
            score += 25
        elif pref_count >= 3:
            score += 20
        elif pref_count >= 1:
            score += 15
        
        # Job count component (30% of score)
        if job_count >= 15:
            score += 30
        elif job_count >= 10:
            score += 25
        elif job_count >= 5:
            score += 20
        elif job_count >= 1:
            score += 15
        
        return min(score, 100.0)

    def _create_preference_stats_document(self, preference_stats: Dict[str, Any], available_data: Dict[str, bool]) -> List[TransformedDocument]:
        """Create document for image preference test statistics with enhanced templates"""
        documents = []
        
        if available_data["stats"]:
            total_count = preference_stats.get('total_image_count', 0)
            response_count = preference_stats.get('response_count', 0)
            response_rate = preference_stats.get('response_rate', 0)
            
            summary = f"ì´ë¯¸ì§€ ì„ í˜¸ë„ ê²€ì‚¬ í†µê³„: ì´ {total_count}ê°œ ì´ë¯¸ì§€ ì¤‘ {response_count}ê°œ ì‘ë‹µ (ì‘ë‹µë¥  {response_rate}%)"
            
            # Enhanced interpretation with actionable insights
            interpretation = self._generate_stats_interpretation(response_rate, total_count, response_count)
            
            # Add recommendations based on completion status
            recommendations = self._generate_stats_recommendations(response_rate)
            
            content = {
                **preference_stats,
                "interpretation": interpretation,
                "recommendations": recommendations,
                "completion_status": "ì™„ë£Œ" if response_rate >= 80 else "ë¶€ë¶„ì™„ë£Œ" if response_rate >= 50 else "ë¯¸ì™„ë£Œ",
                "quality_indicator": self._get_quality_indicator(response_rate),
                "next_steps": self._get_stats_next_steps(response_rate)
            }
            
            documents.append(TransformedDocument(
                doc_type="PREFERENCE_ANALYSIS",
                content=content,
                summary_text=summary,
                metadata={
                    "data_sources": ["imagePreferenceStatsQuery"], 
                    "created_at": datetime.now().isoformat(), 
                    "sub_type": "test_stats",
                    "completion_level": "high" if response_rate >= 80 else "medium" if response_rate >= 50 else "low",
                    "response_rate": response_rate
                }
            ))
        
        return documents

    def _generate_stats_interpretation(self, response_rate: float, total_count: int, response_count: int) -> str:
        """Generate detailed interpretation of test statistics"""
        if response_rate >= 90:
            return (f"ê²€ì‚¬ê°€ ë§¤ìš° ì¶©ì‹¤íˆ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤ ({response_count}/{total_count} ì‘ë‹µ). "
                   "ì´ëŠ” ë§¤ìš° ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì„ í˜¸ë„ ë¶„ì„ ê²°ê³¼ë¥¼ ì œê³µí•  ìˆ˜ ìˆìœ¼ë©°, "
                   "ê°œì¸ì˜ ì„ í˜¸ íŒ¨í„´ì„ ì •í™•í•˜ê²Œ íŒŒì•…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        elif response_rate >= 80:
            return (f"ê²€ì‚¬ê°€ ì¶©ë¶„íˆ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤ ({response_count}/{total_count} ì‘ë‹µ). "
                   "ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì„ í˜¸ë„ ë¶„ì„ ê²°ê³¼ë¥¼ ì œê³µí•  ìˆ˜ ìˆìœ¼ë©°, "
                   "ì£¼ìš” ì„ í˜¸ ê²½í–¥ì„ ëª…í™•í•˜ê²Œ ì‹ë³„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        elif response_rate >= 60:
            return (f"ê²€ì‚¬ê°€ ì–´ëŠ ì •ë„ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤ ({response_count}/{total_count} ì‘ë‹µ). "
                   "ê¸°ë³¸ì ì¸ ì„ í˜¸ë„ ê²½í–¥ì„ íŒŒì•…í•  ìˆ˜ ìˆì§€ë§Œ, "
                   "ë” ì •í™•í•œ ë¶„ì„ì„ ìœ„í•´ì„œëŠ” ì¶”ê°€ ì‘ë‹µì´ ë„ì›€ì´ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        elif response_rate >= 40:
            return (f"ê²€ì‚¬ê°€ ë¶€ë¶„ì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤ ({response_count}/{total_count} ì‘ë‹µ). "
                   "ì¼ë°˜ì ì¸ ì„ í˜¸ ë°©í–¥ì„±ì€ íŒŒì•…í•  ìˆ˜ ìˆì§€ë§Œ, "
                   "ì„¸ë¶€ì ì¸ ì„ í˜¸ë„ ë¶„ì„ì˜ ì •í™•ë„ëŠ” ì œí•œì ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        else:
            return (f"ê²€ì‚¬ ì™„ë£Œë„ê°€ ë‚®ìŠµë‹ˆë‹¤ ({response_count}/{total_count} ì‘ë‹µ). "
                   "í˜„ì¬ ê²°ê³¼ë¡œëŠ” ì„ í˜¸ë„ íŒ¨í„´ì„ ì •í™•íˆ íŒŒì•…í•˜ê¸° ì–´ë ¤ìš°ë©°, "
                   "ì¶”ê°€ ê²€ì‚¬ ì™„ë£Œë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤.")

    def _generate_stats_recommendations(self, response_rate: float) -> List[str]:
        """Generate recommendations based on response rate"""
        if response_rate >= 80:
            return [
                "ì„ í˜¸ë„ ë¶„ì„ ê²°ê³¼ë¥¼ ìì„¸íˆ ê²€í† í•´ë³´ì„¸ìš”",
                "ì¶”ì²œëœ ì§ì—…ë“¤ê³¼ ë³¸ì¸ì˜ ê´€ì‹¬ì‚¬ë¥¼ ë¹„êµí•´ë³´ì„¸ìš”",
                "ë‹¤ë¥¸ ê²€ì‚¬ ê²°ê³¼ì™€ ì¢…í•©í•˜ì—¬ ì§„ë¡œ ë°©í–¥ì„ ì„¤ì •í•´ë³´ì„¸ìš”"
            ]
        elif response_rate >= 60:
            return [
                "í˜„ì¬ ê²°ê³¼ë¥¼ ì°¸ê³ í•˜ë˜, ì¶”ê°€ ê²€ì‚¬ ì™„ë£Œë¥¼ ê³ ë ¤í•´ë³´ì„¸ìš”",
                "ë‹¤ë¥¸ ê²€ì‚¬ ê²°ê³¼ì™€ í•¨ê»˜ ì¢…í•©ì ìœ¼ë¡œ íŒë‹¨í•´ë³´ì„¸ìš”",
                "ê´€ì‹¬ ìˆëŠ” ë¶„ì•¼ì™€ í˜„ì¬ ê²°ê³¼ë¥¼ ë¹„êµí•´ë³´ì„¸ìš”"
            ]
        else:
            return [
                "ê²€ì‚¬ë¥¼ ë” ì™„ë£Œí•˜ì—¬ ì •í™•í•œ ì„ í˜¸ë„ ë¶„ì„ì„ ë°›ì•„ë³´ì„¸ìš”",
                "í˜„ì¬ëŠ” ë‹¤ë¥¸ ê²€ì‚¬ ê²°ê³¼ë¥¼ ìš°ì„ ì ìœ¼ë¡œ ì°¸ê³ í•˜ì„¸ìš”",
                "ì„±í–¥ ë¶„ì„ì´ë‚˜ ì—­ëŸ‰ ë¶„ì„ ê²°ê³¼ë¥¼ ë¨¼ì € í™•ì¸í•´ë³´ì„¸ìš”"
            ]

    def _get_quality_indicator(self, response_rate: float) -> str:
        """Get quality indicator based on response rate"""
        if response_rate >= 90:
            return "ğŸŸ¢ ë§¤ìš° ë†’ìŒ"
        elif response_rate >= 80:
            return "ğŸŸ¢ ë†’ìŒ"
        elif response_rate >= 60:
            return "ğŸŸ¡ ë³´í†µ"
        elif response_rate >= 40:
            return "ğŸŸ  ë‚®ìŒ"
        else:
            return "ğŸ”´ ë§¤ìš° ë‚®ìŒ"

    def _get_stats_next_steps(self, response_rate: float) -> List[str]:
        """Get next steps based on response rate"""
        if response_rate >= 80:
            return [
                "ì„ í˜¸ë„ ë¶„ì„ ìƒì„¸ ê²°ê³¼ í™•ì¸",
                "ì¶”ì²œ ì§ì—… ëª©ë¡ ê²€í† ",
                "ë‹¤ë¥¸ ê²€ì‚¬ ê²°ê³¼ì™€ ë¹„êµ ë¶„ì„"
            ]
        elif response_rate >= 60:
            return [
                "í˜„ì¬ ì„ í˜¸ë„ ê²°ê³¼ ê²€í† ",
                "ì¶”ê°€ ê²€ì‚¬ ì™„ë£Œ ê³ ë ¤",
                "ì„±í–¥ ë¶„ì„ ê²°ê³¼ì™€ ë¹„êµ"
            ]
        else:
            return [
                "ê²€ì‚¬ ì¶”ê°€ ì™„ë£Œ",
                "ë‹¤ë¥¸ ê²€ì‚¬ ê²°ê³¼ ìš°ì„  í™•ì¸",
                "ì„±í–¥ ê¸°ë°˜ ì§ì—… ì¶”ì²œ ê²€í† "
            ]

    def _create_preference_data_documents(self, preference_data: List[Dict[str, Any]], available_data: Dict[str, bool]) -> List[TransformedDocument]:
        """Create enhanced documents for individual preference analysis results"""
        documents = []
        
        if available_data["preferences"]:
            # Create comprehensive overview document
            pref_names = [pref.get('preference_name', '') for pref in preference_data[:3] 
                         if pref and pref.get('preference_name')]
            
            if pref_names:
                overview_summary = f"ì„ í˜¸ë„ ë¶„ì„ ê²°ê³¼: {', '.join(pref_names)} ë“± {len(preference_data)}ê°œ ì„ í˜¸ ì˜ì—­"
            else:
                overview_summary = f"ì„ í˜¸ë„ ë¶„ì„ ê²°ê³¼: {len(preference_data)}ê°œ ì„ í˜¸ ì˜ì—­"
            
            # Generate insights about preference patterns
            insights = self._generate_preference_insights(preference_data)
            
            documents.append(TransformedDocument(
                doc_type="PREFERENCE_ANALYSIS",
                content={
                    "preferences_overview": preference_data,
                    "total_preferences": len(preference_data),
                    "top_preferences": pref_names,
                    "insights": insights,
                    "preference_distribution": self._analyze_preference_distribution(preference_data),
                    "recommendations": self._generate_preference_overview_recommendations(preference_data)
                },
                summary_text=overview_summary,
                metadata={
                    "data_sources": ["preferenceDataQuery"], 
                    "created_at": datetime.now().isoformat(), 
                    "sub_type": "preferences_overview",
                    "completion_level": "high",
                    "preference_count": len(preference_data)
                }
            ))
            
            # Create enhanced individual preference documents
            for i, pref in enumerate(preference_data):
                if not pref:  # Skip None objects
                    continue
                pref_name = pref.get('preference_name')
                if pref_name and pref_name.strip():  # Check for non-empty name
                    rank = pref.get('rank', i + 1)
                    response_rate = pref.get('response_rate', 0)
                    description = pref.get('description', '')
                    
                    summary = f"{pref_name} ì„ í˜¸ë„: {rank}ìˆœìœ„"
                    if response_rate:
                        summary += f", ì‘ë‹µë¥  {response_rate}%"
                    
                    # Enhanced analysis with career implications
                    analysis = self._generate_detailed_preference_analysis(pref_name, rank, description)
                    
                    # Career and development suggestions
                    career_implications = self._generate_career_implications(pref_name, rank)
                    
                    content = {
                        **pref,
                        "rank": rank,
                        "analysis": analysis,
                        "career_implications": career_implications,
                        "preference_strength": "ê°•í•¨" if rank == 1 else "ë³´í†µ" if rank <= 3 else "ì•½í•¨",
                        "development_suggestions": self._generate_development_suggestions(pref_name, rank),
                        "related_activities": self._suggest_related_activities(pref_name)
                    }
                    
                    documents.append(TransformedDocument(
                        doc_type="PREFERENCE_ANALYSIS",
                        content=content,
                        summary_text=summary,
                        metadata={
                            "data_sources": ["preferenceDataQuery"], 
                            "created_at": datetime.now().isoformat(), 
                            "sub_type": f"preference_{rank}",
                            "preference_name": pref_name,
                            "completion_level": "high",
                            "rank": rank
                        }
                    ))
        
        return documents

    def _generate_preference_insights(self, preference_data: List[Dict[str, Any]]) -> List[str]:
        """Generate insights about overall preference patterns"""
        insights = []
        
        # Filter out None objects first
        valid_preferences = [p for p in preference_data if p is not None]
        
        if len(valid_preferences) >= 8:
            insights.append("ë‹¤ì–‘í•œ ì„ í˜¸ ì˜ì—­ì´ ì‹ë³„ë˜ì–´ í­ë„“ì€ ê´€ì‹¬ì‚¬ì™€ ì ì‘ë ¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.")
        elif len(valid_preferences) >= 5:
            insights.append("ì ì ˆí•œ ìˆ˜ì˜ ì„ í˜¸ ì˜ì—­ì´ ìˆì–´ ê· í˜•ì¡íŒ ê´€ì‹¬ì‚¬ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.")
        else:
            insights.append("ëª…í™•í•œ ì„ í˜¸ ì˜ì—­ì´ ìˆì–´ ì§‘ì¤‘ì ì¸ ê´€ì‹¬ì‚¬ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.")
        
        # Analyze top preferences - handle None ranks
        top_prefs = sorted(valid_preferences, key=lambda x: x.get('rank') if x.get('rank') is not None else 999)[:3]
        if top_prefs:
            top_names = [p.get('preference_name', '') for p in top_prefs if p.get('preference_name')]
            if len(top_names) >= 2:
                insights.append(f"ìƒìœ„ ì„ í˜¸ë„ì¸ '{top_names[0]}'ì™€ '{top_names[1]}'ì´ ì£¼ìš” ê´€ì‹¬ ì˜ì—­ì…ë‹ˆë‹¤.")
        
        return insights

    def _analyze_preference_distribution(self, preference_data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Analyze the distribution of preference strengths"""
        # Filter out None objects first
        valid_preferences = [p for p in preference_data if p is not None]
        
        strong_prefs = len([p for p in valid_preferences if (p.get('rank') or 999) <= 2])
        medium_prefs = len([p for p in valid_preferences if 3 <= (p.get('rank') or 999) <= 5])
        weak_prefs = len([p for p in valid_preferences if (p.get('rank') or 999) > 5])
        
        return {
            "strong_preferences": strong_prefs,
            "medium_preferences": medium_prefs,
            "weak_preferences": weak_prefs,
            "total_preferences": len(valid_preferences),
            "concentration_level": "ì§‘ì¤‘í˜•" if strong_prefs >= 3 else "ê· í˜•í˜•" if medium_prefs >= 3 else "ë¶„ì‚°í˜•"
        }

    def _generate_preference_overview_recommendations(self, preference_data: List[Dict[str, Any]]) -> List[str]:
        """Generate recommendations based on overall preference pattern"""
        recommendations = []
        
        # Filter out None objects first
        valid_preferences = [p for p in preference_data if p is not None]
        
        top_prefs = sorted(valid_preferences, key=lambda x: x.get('rank') if x.get('rank') is not None else 999)[:3]
        if top_prefs:
            recommendations.append("ìƒìœ„ ì„ í˜¸ë„ ì˜ì—­ì„ ì¤‘ì‹¬ìœ¼ë¡œ ì§„ë¡œ ë°©í–¥ì„ ì„¤ì •í•´ë³´ì„¸ìš”.")
            recommendations.append("ì„ í˜¸ë„ ê¸°ë°˜ ì§ì—… ì¶”ì²œì„ í™•ì¸í•˜ì—¬ êµ¬ì²´ì ì¸ ì§ì—…ì„ íƒìƒ‰í•´ë³´ì„¸ìš”.")
        
        if len(valid_preferences) >= 5:
            recommendations.append("ë‹¤ì–‘í•œ ì„ í˜¸ ì˜ì—­ì„ í™œìš©í•  ìˆ˜ ìˆëŠ” ìœµí•©ì  ì§ì—…ë„ ê³ ë ¤í•´ë³´ì„¸ìš”.")
        
        recommendations.append("ì„±í–¥ ë¶„ì„ ê²°ê³¼ì™€ ë¹„êµí•˜ì—¬ ì¼ì¹˜í•˜ëŠ” ë¶€ë¶„ì„ í™•ì¸í•´ë³´ì„¸ìš”.")
        
        return recommendations

    def _generate_detailed_preference_analysis(self, pref_name: str, rank: int, description: str) -> str:
        """Generate detailed analysis for individual preferences"""
        base_analysis = ""
        
        if rank == 1:
            base_analysis = f"'{pref_name}'ì€ ê°€ì¥ ê°•í•œ ì„ í˜¸ë¥¼ ë³´ì´ëŠ” ì˜ì—­ì…ë‹ˆë‹¤. "
            base_analysis += "ì´ëŠ” ê°œì¸ì˜ í•µì‹¬ì ì¸ ê´€ì‹¬ì‚¬ì´ì ë™ê¸° ìš”ì†Œë¡œ ì‘ìš©í•  ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤. "
        elif rank <= 3:
            base_analysis = f"'{pref_name}'ì€ ìƒìœ„ ì„ í˜¸ ì˜ì—­ ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤. "
            base_analysis += "ì´ ì˜ì—­ì— ëŒ€í•œ ê´€ì‹¬ê³¼ ì ì„±ì´ ìˆì–´ ê´€ë ¨ í™œë™ì—ì„œ ë§Œì¡±ê°ì„ ëŠë‚„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. "
        elif rank <= 5:
            base_analysis = f"'{pref_name}'ì€ ì¤‘ê°„ ì •ë„ì˜ ì„ í˜¸ë¥¼ ë³´ì´ëŠ” ì˜ì—­ì…ë‹ˆë‹¤. "
            base_analysis += "ìƒí™©ì— ë”°ë¼ ê´€ì‹¬ì„ ê°€ì§ˆ ìˆ˜ ìˆëŠ” ì˜ì—­ìœ¼ë¡œ, ë‹¤ë¥¸ ìš”ì†Œì™€ ê²°í•©í•˜ì—¬ ê³ ë ¤í•´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. "
        else:
            base_analysis = f"'{pref_name}'ì€ ìƒëŒ€ì ìœ¼ë¡œ ë‚®ì€ ì„ í˜¸ë¥¼ ë³´ì´ëŠ” ì˜ì—­ì…ë‹ˆë‹¤. "
            base_analysis += "í˜„ì¬ë¡œì„œëŠ” ì£¼ìš” ê´€ì‹¬ì‚¬ê°€ ì•„ë‹ˆì§€ë§Œ, í–¥í›„ ê²½í—˜ì„ í†µí•´ ë³€í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. "
        
        if description:
            base_analysis += f"êµ¬ì²´ì ìœ¼ë¡œëŠ” {description}"
        
        return base_analysis

    def _generate_career_implications(self, pref_name: str, rank: int) -> List[str]:
        """Generate career implications based on preference"""
        implications = []
        
        if rank <= 2:
            implications.append(f"{pref_name} ê´€ë ¨ ì§ì—…ì„ ìš°ì„ ì ìœ¼ë¡œ ê³ ë ¤í•´ë³´ì„¸ìš”.")
            implications.append("ì´ ì˜ì—­ì—ì„œ ì „ë¬¸ì„±ì„ ê°œë°œí•˜ë©´ ë†’ì€ ë§Œì¡±ë„ë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        elif rank <= 5:
            implications.append(f"{pref_name} ìš”ì†Œê°€ í¬í•¨ëœ ì§ì—…ì„ íƒìƒ‰í•´ë³´ì„¸ìš”.")
            implications.append("ì£¼ ì—…ë¬´ê°€ ì•„ë‹ˆë”ë¼ë„ ë¶€ë¶„ì ìœ¼ë¡œ ê´€ë ¨ëœ ì—­í• ì„ ì°¾ì•„ë³´ì„¸ìš”.")
        
        return implications

    def _generate_development_suggestions(self, pref_name: str, rank: int) -> List[str]:
        """Generate development suggestions based on preference"""
        suggestions = []
        
        if rank <= 3:
            suggestions.append(f"{pref_name} ê´€ë ¨ ì—­ëŸ‰ì„ ë”ìš± ë°œì „ì‹œì¼œë³´ì„¸ìš”.")
            suggestions.append("ê´€ë ¨ êµìœ¡ì´ë‚˜ ê²½í—˜ ê¸°íšŒë¥¼ ì ê·¹ì ìœ¼ë¡œ ì°¾ì•„ë³´ì„¸ìš”.")
            suggestions.append("ì´ ì˜ì—­ì˜ ì „ë¬¸ê°€ë‚˜ ë©˜í† ë¥¼ ì°¾ì•„ ì¡°ì–¸ì„ êµ¬í•´ë³´ì„¸ìš”.")
        else:
            suggestions.append("ë‹¤ë¥¸ ê°•ì  ì˜ì—­ì— ë” ì§‘ì¤‘í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤.")
            suggestions.append("í•„ìš”ì‹œ ê¸°ë³¸ì ì¸ ì´í•´ ìˆ˜ì¤€ìœ¼ë¡œ í•™ìŠµí•´ë³´ì„¸ìš”.")
        
        return suggestions

    def _suggest_related_activities(self, pref_name: str) -> List[str]:
        """Suggest activities related to the preference"""
        # This could be enhanced with a more sophisticated mapping
        activities = []
        
        if "ì‹¤ë‚´" in pref_name or "ì¡°ìš©" in pref_name:
            activities.extend(["ë…ì„œ", "ì—°êµ¬", "ë¶„ì„ ì‘ì—…", "ê³„íš ìˆ˜ë¦½"])
        elif "ì°½ì˜" in pref_name or "ì˜ˆìˆ " in pref_name:
            activities.extend(["ë””ìì¸", "ê¸€ì“°ê¸°", "ì•„ì´ë””ì–´ ë°œìƒ", "ì˜ˆìˆ  í™œë™"])
        elif "ì‚¬ëŒ" in pref_name or "ì†Œí†µ" in pref_name:
            activities.extend(["íŒ€ í”„ë¡œì íŠ¸", "ë°œí‘œ", "ìƒë‹´", "êµìœ¡"])
        elif "ì•¼ì™¸" in pref_name or "í™œë™" in pref_name:
            activities.extend(["í˜„ì¥ ì—…ë¬´", "ì²´í—˜ í™œë™", "ì—¬í–‰", "ìš´ë™"])
        else:
            activities.extend(["ê´€ë ¨ ì²´í—˜", "í•™ìŠµ", "íƒìƒ‰"])
        
        return activities

    def _create_preference_jobs_documents(self, preference_jobs: List[Dict[str, Any]], available_data: Dict[str, bool]) -> List[TransformedDocument]:
        """Create enhanced documents for preference-based job recommendations"""
        documents = []
        
        if available_data["jobs"]:
            # Group jobs by preference type
            jobs_by_preference = {}
            for job in preference_jobs:
                if not job:  # Skip None objects
                    continue
                pref_type = job.get('preference_type', 'unknown')
                pref_name = job.get('preference_name')
                if not pref_name or not pref_name.strip():
                    pref_name = f'ì„ í˜¸ë„ {pref_type}'
                
                if pref_name not in jobs_by_preference:
                    jobs_by_preference[pref_name] = []
                jobs_by_preference[pref_name].append(job)
            
            # Create overview document for all job recommendations
            if jobs_by_preference:
                total_jobs = sum(len(jobs) for jobs in jobs_by_preference.values())
                pref_types = list(jobs_by_preference.keys())
                
                overview_summary = f"ì„ í˜¸ë„ ê¸°ë°˜ ì§ì—… ì¶”ì²œ: {len(pref_types)}ê°œ ì„ í˜¸ ì˜ì—­ì—ì„œ ì´ {total_jobs}ê°œ ì§ì—…"
                
                documents.append(TransformedDocument(
                    doc_type="PREFERENCE_ANALYSIS",
                    content={
                        "total_jobs": total_jobs,
                        "preference_types": pref_types,
                        "jobs_by_preference": jobs_by_preference,
                        "overview_insights": self._generate_job_overview_insights(jobs_by_preference),
                        "career_diversity": self._assess_career_diversity(jobs_by_preference),
                        "recommendations": self._generate_job_overview_recommendations(jobs_by_preference)
                    },
                    summary_text=overview_summary,
                    metadata={
                        "data_sources": ["preferenceJobsQuery"], 
                        "created_at": datetime.now().isoformat(), 
                        "sub_type": "jobs_overview",
                        "completion_level": "high",
                        "job_count": total_jobs,
                        "preference_count": len(pref_types)
                    }
                ))
            
            # Create detailed documents for each preference type
            for pref_name, jobs in jobs_by_preference.items():
                job_names = [job.get('jo_name', '') for job in jobs[:3] if job.get('jo_name')]
                summary = f"{pref_name} ê¸°ë°˜ ì¶”ì²œ ì§ì—…: {', '.join(job_names)}"
                if len(jobs) > 3:
                    summary += f" ë“± {len(jobs)}ê°œ"
                
                # Enhanced analysis of job recommendations
                analysis = self._generate_comprehensive_job_analysis(pref_name, jobs)
                
                # Career path suggestions
                career_paths = self._suggest_career_paths(jobs)
                
                # Industry analysis
                industry_analysis = self._analyze_job_industries(jobs)
                
                content = {
                    "preference_name": pref_name,
                    "jobs": jobs,
                    "job_count": len(jobs),
                    "analysis": analysis,
                    "top_jobs": job_names,
                    "career_paths": career_paths,
                    "industry_analysis": industry_analysis,
                    "skill_requirements": self._extract_skill_requirements(jobs),
                    "education_recommendations": self._extract_education_recommendations(jobs),
                    "next_steps": self._suggest_job_exploration_steps(pref_name, jobs)
                }
                
                documents.append(TransformedDocument(
                    doc_type="PREFERENCE_ANALYSIS",
                    content=content,
                    summary_text=summary,
                    metadata={
                        "data_sources": ["preferenceJobsQuery"], 
                        "created_at": datetime.now().isoformat(), 
                        "sub_type": f"jobs_{pref_name.replace(' ', '_')}",
                        "preference_name": pref_name,
                        "completion_level": "high",
                        "job_count": len(jobs)
                    }
                ))
        
        return documents

    def _generate_job_overview_insights(self, jobs_by_preference: Dict[str, List[Dict[str, Any]]]) -> List[str]:
        """Generate insights about overall job recommendation patterns"""
        insights = []
        
        total_jobs = sum(len(jobs) for jobs in jobs_by_preference.values())
        pref_count = len(jobs_by_preference)
        
        if total_jobs >= 20:
            insights.append("ë§¤ìš° ë‹¤ì–‘í•œ ì§ì—… ì˜µì…˜ì´ ì œì‹œë˜ì–´ ì„ íƒì˜ í­ì´ ë„“ìŠµë‹ˆë‹¤.")
        elif total_jobs >= 10:
            insights.append("ì ì ˆí•œ ìˆ˜ì˜ ì§ì—… ì˜µì…˜ì´ ìˆì–´ êµ¬ì²´ì ì¸ íƒìƒ‰ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
        else:
            insights.append("ëª…í™•í•œ ì§ì—… ë°©í–¥ì„±ì´ ì œì‹œë˜ì–´ ì§‘ì¤‘ì ì¸ íƒìƒ‰ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
        
        if pref_count >= 4:
            insights.append("ì—¬ëŸ¬ ì„ í˜¸ ì˜ì—­ì—ì„œ ì§ì—…ì´ ì¶”ì²œë˜ì–´ ë‹¤ì–‘í•œ ê´€ì‹¬ì‚¬ë¥¼ ë°˜ì˜í•©ë‹ˆë‹¤.")
        
        return insights

    def _assess_career_diversity(self, jobs_by_preference: Dict[str, List[Dict[str, Any]]]) -> Dict[str, Any]:
        """Assess the diversity of career recommendations"""
        all_jobs = []
        for jobs in jobs_by_preference.values():
            all_jobs.extend(jobs)
        
        # Extract industries (simplified)
        industries = set()
        for job in all_jobs:
            outline = job.get('jo_outline', '')
            if outline:
                industries.add(outline)
        
        return {
            "total_jobs": len(all_jobs),
            "unique_industries": len(industries),
            "diversity_score": min(len(industries) / max(len(all_jobs), 1) * 100, 100),
            "diversity_level": "ë†’ìŒ" if len(industries) >= 8 else "ë³´í†µ" if len(industries) >= 4 else "ë‚®ìŒ"
        }

    def _generate_job_overview_recommendations(self, jobs_by_preference: Dict[str, List[Dict[str, Any]]]) -> List[str]:
        """Generate recommendations for job exploration"""
        recommendations = []
        
        # Find preference with most jobs
        max_jobs_pref = max(jobs_by_preference.items(), key=lambda x: len(x[1]))
        recommendations.append(f"'{max_jobs_pref[0]}' ì˜ì—­ì—ì„œ ê°€ì¥ ë§ì€ ì§ì—…ì´ ì¶”ì²œë˜ë¯€ë¡œ ìš°ì„ ì ìœ¼ë¡œ íƒìƒ‰í•´ë³´ì„¸ìš”.")
        
        recommendations.extend([
            "ê° ì„ í˜¸ ì˜ì—­ë³„ ì¶”ì²œ ì§ì—…ì„ ìì„¸íˆ ê²€í† í•´ë³´ì„¸ìš”.",
            "ê´€ì‹¬ ìˆëŠ” ì§ì—…ì˜ êµ¬ì²´ì ì¸ ì—…ë¬´ ë‚´ìš©ì„ ì¡°ì‚¬í•´ë³´ì„¸ìš”.",
            "ì¶”ì²œ ì „ê³µê³¼ í˜„ì¬ ì „ê³µ/ê´€ì‹¬ ë¶„ì•¼ë¥¼ ë¹„êµí•´ë³´ì„¸ìš”.",
            "ì„±í–¥ ê¸°ë°˜ ì§ì—… ì¶”ì²œê³¼ ë¹„êµí•˜ì—¬ ì¼ì¹˜í•˜ëŠ” ì§ì—…ì„ ì°¾ì•„ë³´ì„¸ìš”."
        ])
        
        return recommendations

    def _generate_comprehensive_job_analysis(self, pref_name: str, jobs: List[Dict[str, Any]]) -> str:
        """Generate comprehensive analysis of job recommendations for a preference"""
        analysis = f"'{pref_name}' ì„ í˜¸ë„ë¥¼ ë°”íƒ•ìœ¼ë¡œ {len(jobs)}ê°œì˜ ì§ì—…ì´ ì¶”ì²œë˜ì—ˆìŠµë‹ˆë‹¤. "
        
        if len(jobs) >= 8:
            analysis += "ë§¤ìš° ë‹¤ì–‘í•œ ì§ì—… ì˜µì…˜ì´ ìˆì–´ ì„ íƒì˜ í­ì´ ë„“ê³ , ì´ ì„ í˜¸ë„ê°€ ì—¬ëŸ¬ ë¶„ì•¼ì—ì„œ í™œìš©ë  ìˆ˜ ìˆìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. "
        elif len(jobs) >= 4:
            analysis += "ì ì ˆí•œ ìˆ˜ì˜ ì§ì—… ì˜µì…˜ì´ ì œê³µë˜ì–´ êµ¬ì²´ì ì¸ ì§„ë¡œ íƒìƒ‰ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤. "
        else:
            analysis += "ëª…í™•í•œ ì§ì—… ë°©í–¥ì„±ì´ ì œì‹œë˜ì–´ ì§‘ì¤‘ì ì¸ íƒìƒ‰ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤. "
        
        # Analyze job types
        job_outlines = [job.get('jo_outline', '') for job in jobs if job.get('jo_outline')]
        if job_outlines:
            unique_outlines = set(job_outlines)
            if len(unique_outlines) >= 5:
                analysis += "ë‹¤ì–‘í•œ ì—…ë¬´ ì˜ì—­ì— ê±¸ì³ ì¶”ì²œë˜ì–´ í­ë„“ì€ ì ìš© ê°€ëŠ¥ì„±ì„ ë³´ì—¬ì¤ë‹ˆë‹¤."
            else:
                analysis += "íŠ¹ì • ì—…ë¬´ ì˜ì—­ì— ì§‘ì¤‘ë˜ì–´ ëª…í™•í•œ ì „ë¬¸ì„± ë°©í–¥ì„ ì œì‹œí•©ë‹ˆë‹¤."
        
        return analysis

    def _suggest_career_paths(self, jobs: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Suggest career paths based on job recommendations"""
        paths = []
        
        # Group jobs by similar characteristics
        job_groups = {}
        for job in jobs:
            outline = job.get('jo_outline', 'ê¸°íƒ€')
            if outline not in job_groups:
                job_groups[outline] = []
            job_groups[outline].append(job)
        
        for outline, group_jobs in job_groups.items():
            if len(group_jobs) >= 2:  # Only suggest paths with multiple jobs
                paths.append({
                    "path_name": f"{outline} ë¶„ì•¼",
                    "jobs": [job.get('jo_name', '') for job in group_jobs],
                    "description": f"{outline} ì˜ì—­ì—ì„œì˜ ë‹¤ì–‘í•œ ì§ì—… ê¸°íšŒ"
                })
        
        return paths

    def _analyze_job_industries(self, jobs: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Analyze industries represented in job recommendations"""
        industries = {}
        for job in jobs:
            outline = job.get('jo_outline', 'ê¸°íƒ€')
            if outline not in industries:
                industries[outline] = []
            industries[outline].append(job.get('jo_name', ''))
        
        return {
            "industry_count": len(industries),
            "industries": industries,
            "dominant_industry": max(industries.items(), key=lambda x: len(x[1]))[0] if industries else None
        }

    def _extract_skill_requirements(self, jobs: List[Dict[str, Any]]) -> List[str]:
        """Extract common skill requirements from job recommendations"""
        skills = set()
        
        for job in jobs:
            # Extract from job main business
            main_business = job.get('jo_mainbusiness', '')
            if main_business:
                # Simple keyword extraction (could be enhanced)
                if 'ë¶„ì„' in main_business:
                    skills.add('ë¶„ì„ ëŠ¥ë ¥')
                if 'ì„¤ê³„' in main_business:
                    skills.add('ì„¤ê³„ ëŠ¥ë ¥')
                if 'ê°œë°œ' in main_business:
                    skills.add('ê°œë°œ ëŠ¥ë ¥')
                if 'ê´€ë¦¬' in main_business:
                    skills.add('ê´€ë¦¬ ëŠ¥ë ¥')
                if 'ì†Œí†µ' in main_business or 'ìƒë‹´' in main_business:
                    skills.add('ì†Œí†µ ëŠ¥ë ¥')
        
        return list(skills)

    def _extract_education_recommendations(self, jobs: List[Dict[str, Any]]) -> List[str]:
        """Extract education recommendations from job data"""
        majors = set()
        
        for job in jobs:
            major_info = job.get('majors', '')
            if major_info:
                # Split by common delimiters
                for delimiter in [',', '/', 'Â·', 'ë°']:
                    if delimiter in major_info:
                        major_parts = major_info.split(delimiter)
                        for part in major_parts:
                            clean_major = part.strip()
                            if clean_major:
                                majors.add(clean_major)
                        break
                else:
                    majors.add(major_info.strip())
        
        return list(majors)

    def _suggest_job_exploration_steps(self, pref_name: str, jobs: List[Dict[str, Any]]) -> List[str]:
        """Suggest specific steps for exploring these job recommendations"""
        steps = []
        
        if len(jobs) >= 5:
            steps.append("ê´€ì‹¬ ìˆëŠ” ìƒìœ„ 3-5ê°œ ì§ì—…ì„ ì„ ë³„í•´ë³´ì„¸ìš”.")
        else:
            steps.append("ëª¨ë“  ì¶”ì²œ ì§ì—…ì„ ìì„¸íˆ ê²€í† í•´ë³´ì„¸ìš”.")
        
        steps.extend([
            "ê° ì§ì—…ì˜ êµ¬ì²´ì ì¸ ì—…ë¬´ ë‚´ìš©ê³¼ ìš”êµ¬ ì—­ëŸ‰ì„ ì¡°ì‚¬í•´ë³´ì„¸ìš”.",
            "í•´ë‹¹ ë¶„ì•¼ ì¢…ì‚¬ìì™€ì˜ ì¸í„°ë·°ë‚˜ ë©˜í† ë§ì„ ê³ ë ¤í•´ë³´ì„¸ìš”.",
            "ê´€ë ¨ êµìœ¡ ê³¼ì •ì´ë‚˜ ìê²©ì¦ ì •ë³´ë¥¼ í™•ì¸í•´ë³´ì„¸ìš”.",
            "ì¸í„´ì‹­ì´ë‚˜ ì²´í—˜ í”„ë¡œê·¸ë¨ ê¸°íšŒë¥¼ ì°¾ì•„ë³´ì„¸ìš”."
        ])
        
        return steps

    def _create_preference_fallback_document(self, available_data: Dict[str, bool]) -> TransformedDocument:
        """Create informative fallback document when preference data is missing"""
        
        # Determine what specific data is missing and why
        missing_components = []
        if not available_data["stats"]:
            missing_components.append("ì´ë¯¸ì§€ ì„ í˜¸ë„ ê²€ì‚¬ í†µê³„")
        if not available_data["preferences"]:
            missing_components.append("ì„ í˜¸ë„ ë¶„ì„ ê²°ê³¼")
        if not available_data["jobs"]:
            missing_components.append("ì„ í˜¸ë„ ê¸°ë°˜ ì§ì—… ì¶”ì²œ")
        
        # Create helpful explanation based on what's missing
        explanation = self._generate_missing_data_explanation(missing_components)
        
        # Suggest alternatives based on available test results
        alternatives = self._generate_alternative_suggestions()
        
        # Provide specific recommendations
        recommendation = self._generate_specific_recommendations(missing_components)
        
        content = {
            "status": "ë°ì´í„° ì¤€ë¹„ ì¤‘",
            "missing_components": missing_components,
            "explanation": explanation,
            "alternatives": alternatives,
            "recommendation": recommendation,
            "data_availability": self._assess_data_availability(available_data),
            "next_steps": self._suggest_next_steps(missing_components)
        }
        
        return TransformedDocument(
            doc_type="PREFERENCE_ANALYSIS",
            content=content,
            summary_text="ì„ í˜¸ë„ ë¶„ì„: ë°ì´í„° ì¤€ë¹„ ì¤‘ - ë‹¤ë¥¸ ë¶„ì„ ê²°ê³¼ ì´ìš© ê°€ëŠ¥",
            metadata={
                "data_sources": [], 
                "created_at": datetime.now().isoformat(), 
                "sub_type": "unavailable",
                "completion_level": "none",
                "has_alternatives": True,
                "missing_count": len(missing_components)
            }
        )

    def _generate_missing_data_explanation(self, missing_components: List[str]) -> str:
        """Generate detailed explanation for why preference data might be missing"""
        if len(missing_components) == 3:
            # All preference data is missing
            explanation = "í˜„ì¬ ì´ë¯¸ì§€ ì„ í˜¸ë„ ë¶„ì„ê³¼ ê´€ë ¨ëœ ëª¨ë“  ë°ì´í„°ë¥¼ ì´ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n\n"
            explanation += "ì´ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ì´ìœ ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n"
            explanation += "â€¢ ì´ë¯¸ì§€ ì„ í˜¸ë„ ê²€ì‚¬ë¥¼ ì•„ì§ ì‹œì‘í•˜ì§€ ì•Šì•˜ê±°ë‚˜ ì™„ë£Œí•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤\n"
            explanation += "â€¢ ê²€ì‚¬ëŠ” ì™„ë£Œí–ˆì§€ë§Œ ê²°ê³¼ ì²˜ë¦¬ê°€ ì•„ì§ ì§„í–‰ ì¤‘ì…ë‹ˆë‹¤\n"
            explanation += "â€¢ ê²€ì‚¬ ì‘ë‹µë¥ ì´ ë‚®ì•„ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ë¶„ì„ì´ ì–´ë µìŠµë‹ˆë‹¤\n"
            explanation += "â€¢ ì¼ì‹œì ì¸ ì‹œìŠ¤í…œ ì²˜ë¦¬ ì§€ì—°ì´ ë°œìƒí–ˆìŠµë‹ˆë‹¤\n"
            explanation += "â€¢ ê²€ì‚¬ ë°ì´í„°ì— ì˜¤ë¥˜ê°€ ìˆì–´ ì¬ì²˜ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤"
        elif len(missing_components) == 2:
            # Partial data missing
            explanation = f"í˜„ì¬ ë‹¤ìŒ ì„ í˜¸ë„ ë¶„ì„ ë°ì´í„°ë¥¼ ì´ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤:\n"
            explanation += "\n".join([f"â€¢ {component}" for component in missing_components])
            explanation += "\n\nì´ëŠ” ê²€ì‚¬ê°€ ë¶€ë¶„ì ìœ¼ë¡œë§Œ ì™„ë£Œë˜ì—ˆê±°ë‚˜, ì¼ë¶€ ë°ì´í„° ì²˜ë¦¬ê°€ ì§€ì—°ë˜ê³  ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        else:
            # Single component missing
            component = missing_components[0]
            explanation = f"í˜„ì¬ {component} ë°ì´í„°ë¥¼ ì´ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n\n"
            if "í†µê³„" in component:
                explanation += "ê²€ì‚¬ í†µê³„ ì •ë³´ëŠ” ì²˜ë¦¬ ì¤‘ì´ì§€ë§Œ, ë‹¤ë¥¸ ì„ í˜¸ë„ ë¶„ì„ ê²°ê³¼ëŠ” í™•ì¸í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
            elif "ë¶„ì„ ê²°ê³¼" in component:
                explanation += "ì„ í˜¸ë„ ë¶„ì„ì€ ì²˜ë¦¬ ì¤‘ì´ì§€ë§Œ, ê²€ì‚¬ í†µê³„ì™€ ì§ì—… ì¶”ì²œì€ í™•ì¸í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
            else:
                explanation += "ì§ì—… ì¶”ì²œì€ ì²˜ë¦¬ ì¤‘ì´ì§€ë§Œ, ë‹¤ë¥¸ ì„ í˜¸ë„ ë¶„ì„ ê²°ê³¼ëŠ” í™•ì¸í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        
        return explanation

    def _generate_alternative_suggestions(self) -> str:
        """Generate suggestions for alternative test results to explore"""
        alternatives = "\nëŒ€ì‹  ë‹¤ìŒ ë¶„ì„ ê²°ê³¼ë¥¼ í™•ì¸í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n\n"
        alternatives += "ğŸ” **ì„±í–¥ ë¶„ì„**\n"
        alternatives += "   â€¢ ê°œì¸ì˜ ì„±ê²© ìœ í˜•ê³¼ í–‰ë™ íŒ¨í„´ ë¶„ì„\n"
        alternatives += "   â€¢ ì£¼ìš” ì„±í–¥ê³¼ íŠ¹ì„±ì— ëŒ€í•œ ìƒì„¸í•œ ì„¤ëª…\n"
        alternatives += "   â€¢ ì„±í–¥ ê¸°ë°˜ ê°•ì ê³¼ ê°œì„  ì˜ì—­ íŒŒì•…\n\n"
        
        alternatives += "ğŸ§  **ì‚¬ê³ ë ¥ ë¶„ì„**\n"
        alternatives += "   â€¢ ë‹¤ì–‘í•œ ì¸ì§€ ëŠ¥ë ¥ê³¼ ì‚¬ê³  ìŠ¤íƒ€ì¼ í‰ê°€\n"
        alternatives += "   â€¢ ë…¼ë¦¬ì , ì°½ì˜ì , ë¶„ì„ì  ì‚¬ê³ ë ¥ ì¸¡ì •\n"
        alternatives += "   â€¢ ê°œì¸ë³„ ì‚¬ê³  ê°•ì  ì˜ì—­ ì‹ë³„\n\n"
        
        alternatives += "ğŸ’ª **ì—­ëŸ‰ ë¶„ì„**\n"
        alternatives += "   â€¢ í•µì‹¬ ì—­ëŸ‰ê³¼ ëŠ¥ë ¥ í‰ê°€\n"
        alternatives += "   â€¢ ì§ë¬´ë³„ ì í•©ì„±ê³¼ ì ì¬ë ¥ ë¶„ì„\n"
        alternatives += "   â€¢ ê°œë°œ ê°€ëŠ¥í•œ ì—­ëŸ‰ ì˜ì—­ ì œì‹œ\n\n"
        
        alternatives += "ğŸ’¼ **ì§ì—… ì¶”ì²œ**\n"
        alternatives += "   â€¢ ì„±í–¥ê³¼ ì—­ëŸ‰ ê¸°ë°˜ ì§ì—… ì¶”ì²œ\n"
        alternatives += "   â€¢ ì í•©í•œ ì§ë¬´ì™€ ì—…ë¬´ í™˜ê²½ ì œì•ˆ\n"
        alternatives += "   â€¢ ê´€ë ¨ ì „ê³µê³¼ í•™ìŠµ ë°©í–¥ ì•ˆë‚´\n\n"
        
        alternatives += "ğŸ“š **í•™ìŠµ ìŠ¤íƒ€ì¼**\n"
        alternatives += "   â€¢ ê°œì¸ì—ê²Œ ë§ëŠ” í•™ìŠµ ë°©ë²• ì œì•ˆ\n"
        alternatives += "   â€¢ íš¨ê³¼ì ì¸ ê³µë¶€ ì „ëµê³¼ í™˜ê²½ ì¶”ì²œ\n"
        alternatives += "   â€¢ ì¶”ì²œ í•™ìŠµ ê³¼ëª©ê³¼ ë¶„ì•¼ ì•ˆë‚´"
        
        return alternatives

    def _generate_specific_recommendations(self, missing_components: List[str]) -> str:
        """Generate specific recommendations based on what's missing"""
        if len(missing_components) == 3:
            return ("ì´ë¯¸ì§€ ì„ í˜¸ë„ ê²€ì‚¬ë¥¼ ì™„ë£Œí•˜ì§€ ì•Šìœ¼ì…¨ë‹¤ë©´ ë¨¼ì € ê²€ì‚¬ë¥¼ ì§„í–‰í•´ë³´ì„¸ìš”. "
                   "ê²€ì‚¬ë¥¼ ì™„ë£Œí•˜ì…¨ë‹¤ë©´ ì ì‹œ í›„ ë‹¤ì‹œ í™•ì¸í•´ë³´ì‹œê±°ë‚˜, "
                   "ë‹¤ë¥¸ ë¶„ì„ ê²°ê³¼ë¥¼ ë¨¼ì € ì‚´í´ë³´ì‹œëŠ” ê²ƒì„ ì¶”ì²œí•©ë‹ˆë‹¤.")
        elif len(missing_components) == 2:
            return ("ì¼ë¶€ ì„ í˜¸ë„ ë°ì´í„°ëŠ” ì²˜ë¦¬ ì¤‘ì…ë‹ˆë‹¤. "
                   "ì´ìš© ê°€ëŠ¥í•œ ë‹¤ë¥¸ ë¶„ì„ ê²°ê³¼ë¥¼ ë¨¼ì € í™•ì¸í•´ë³´ì‹œê³ , "
                   "ì„ í˜¸ë„ ë¶„ì„ì€ ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ë³´ì„¸ìš”.")
        else:
            return ("ëŒ€ë¶€ë¶„ì˜ ì„ í˜¸ë„ ë¶„ì„ ê²°ê³¼ëŠ” ì´ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤. "
                   "í˜„ì¬ ì´ìš© ê°€ëŠ¥í•œ ê²°ê³¼ë¥¼ ë¨¼ì € í™•ì¸í•´ë³´ì‹œê³ , "
                   "ëˆ„ë½ëœ ë¶€ë¶„ì€ ì ì‹œ í›„ ë‹¤ì‹œ í™•ì¸í•´ë³´ì„¸ìš”.")

    def _assess_data_availability(self, available_data: Dict[str, bool]) -> Dict[str, str]:
        """Assess and describe the availability of each data component"""
        availability = {}
        
        if available_data["stats"]:
            availability["ê²€ì‚¬_í†µê³„"] = "ì´ìš© ê°€ëŠ¥"
        else:
            availability["ê²€ì‚¬_í†µê³„"] = "ì²˜ë¦¬ ì¤‘"
            
        if available_data["preferences"]:
            availability["ì„ í˜¸ë„_ë¶„ì„"] = "ì´ìš© ê°€ëŠ¥"
        else:
            availability["ì„ í˜¸ë„_ë¶„ì„"] = "ì²˜ë¦¬ ì¤‘"
            
        if available_data["jobs"]:
            availability["ì§ì—…_ì¶”ì²œ"] = "ì´ìš© ê°€ëŠ¥"
        else:
            availability["ì§ì—…_ì¶”ì²œ"] = "ì²˜ë¦¬ ì¤‘"
            
        return availability

    def _suggest_next_steps(self, missing_components: List[str]) -> List[str]:
        """Suggest specific next steps based on missing data"""
        steps = []
        
        if len(missing_components) == 3:
            steps.extend([
                "ì´ë¯¸ì§€ ì„ í˜¸ë„ ê²€ì‚¬ ì™„ë£Œ ì—¬ë¶€ë¥¼ í™•ì¸í•´ë³´ì„¸ìš”",
                "ì„±í–¥ ë¶„ì„ ê²°ê³¼ë¶€í„° í™•ì¸í•´ë³´ì„¸ìš”",
                "ì‚¬ê³ ë ¥ ë¶„ì„ìœ¼ë¡œ ì¸ì§€ ëŠ¥ë ¥ì„ íŒŒì•…í•´ë³´ì„¸ìš”",
                "ì—­ëŸ‰ ë¶„ì„ìœ¼ë¡œ ê°•ì  ì˜ì—­ì„ í™•ì¸í•´ë³´ì„¸ìš”",
                "30ë¶„ í›„ ì„ í˜¸ë„ ë¶„ì„ì„ ë‹¤ì‹œ ì‹œë„í•´ë³´ì„¸ìš”"
            ])
        elif "ì´ë¯¸ì§€ ì„ í˜¸ë„ ê²€ì‚¬ í†µê³„" in missing_components:
            steps.extend([
                "ì´ìš© ê°€ëŠ¥í•œ ì„ í˜¸ë„ ë¶„ì„ ê²°ê³¼ë¥¼ ë¨¼ì € í™•ì¸í•´ë³´ì„¸ìš”",
                "ê²€ì‚¬ í†µê³„ëŠ” ì ì‹œ í›„ ë‹¤ì‹œ í™•ì¸í•´ë³´ì„¸ìš”"
            ])
        elif "ì„ í˜¸ë„ ë¶„ì„ ê²°ê³¼" in missing_components:
            steps.extend([
                "ê²€ì‚¬ í†µê³„ë¥¼ í†µí•´ ê²€ì‚¬ ì™„ë£Œ ìƒíƒœë¥¼ í™•ì¸í•´ë³´ì„¸ìš”",
                "ì„ í˜¸ë„ ê¸°ë°˜ ì§ì—… ì¶”ì²œì„ ë¨¼ì € ì‚´í´ë³´ì„¸ìš”"
            ])
        else:
            steps.extend([
                "í˜„ì¬ ì´ìš© ê°€ëŠ¥í•œ ì„ í˜¸ë„ ë¶„ì„ì„ í™•ì¸í•´ë³´ì„¸ìš”",
                "ì„±í–¥ ê¸°ë°˜ ì§ì—… ì¶”ì²œê³¼ ë¹„êµí•´ë³´ì„¸ìš”"
            ])
            
        return steps

    def _create_partial_preference_document(self, available_data: Dict[str, bool], partial_content: Dict[str, Any]) -> TransformedDocument:
        """Create document for scenarios with partial preference data"""
        
        available_components = []
        missing_components = []
        
        if available_data["stats"]:
            available_components.append("ì´ë¯¸ì§€ ì„ í˜¸ë„ ê²€ì‚¬ í†µê³„")
        else:
            missing_components.append("ì´ë¯¸ì§€ ì„ í˜¸ë„ ê²€ì‚¬ í†µê³„")
            
        if available_data["preferences"]:
            available_components.append("ì„ í˜¸ë„ ë¶„ì„ ê²°ê³¼")
        else:
            missing_components.append("ì„ í˜¸ë„ ë¶„ì„ ê²°ê³¼")
            
        if available_data["jobs"]:
            available_components.append("ì„ í˜¸ë„ ê¸°ë°˜ ì§ì—… ì¶”ì²œ")
        else:
            missing_components.append("ì„ í˜¸ë„ ê¸°ë°˜ ì§ì—… ì¶”ì²œ")
        
        # Create informative content about partial availability
        explanation = f"ì„ í˜¸ë„ ë¶„ì„ì´ ë¶€ë¶„ì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\n\n"
        explanation += f"**ì´ìš© ê°€ëŠ¥í•œ ë°ì´í„°:**\n"
        explanation += "\n".join([f"âœ… {comp}" for comp in available_components])
        explanation += f"\n\n**ì²˜ë¦¬ ì¤‘ì¸ ë°ì´í„°:**\n"
        explanation += "\n".join([f"â³ {comp}" for comp in missing_components])
        
        explanation += "\n\ní˜„ì¬ ì´ìš© ê°€ëŠ¥í•œ ë°ì´í„°ë¡œë„ ì˜ë¯¸ ìˆëŠ” ì„ í˜¸ë„ ë¶„ì„ì„ ì œê³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. "
        explanation += "ëˆ„ë½ëœ ë°ì´í„°ëŠ” ì²˜ë¦¬ê°€ ì™„ë£Œë˜ëŠ” ëŒ€ë¡œ ì¶”ê°€ë  ì˜ˆì •ì…ë‹ˆë‹¤."
        
        content = {
            "status": "ë¶€ë¶„ ì™„ë£Œ",
            "available_components": available_components,
            "missing_components": missing_components,
            "explanation": explanation,
            "partial_data": partial_content,
            "completion_percentage": (len(available_components) / 3) * 100,
            "recommendation": "í˜„ì¬ ì´ìš© ê°€ëŠ¥í•œ ì„ í˜¸ë„ ë¶„ì„ì„ ë¨¼ì € í™•ì¸í•´ë³´ì‹œê³ , ì¶”ê°€ ë°ì´í„°ëŠ” ì ì‹œ í›„ ë‹¤ì‹œ í™•ì¸í•´ë³´ì„¸ìš”."
        }
        
        summary = f"ì„ í˜¸ë„ ë¶„ì„: ë¶€ë¶„ ì™„ë£Œ ({len(available_components)}/3 í•­ëª© ì´ìš© ê°€ëŠ¥)"
        
        return TransformedDocument(
            doc_type="PREFERENCE_ANALYSIS",
            content=content,
            summary_text=summary,
            metadata={
                "data_sources": [], 
                "created_at": datetime.now().isoformat(), 
                "sub_type": "partial_available",
                "completion_level": "partial",
                "available_count": len(available_components),
                "missing_count": len(missing_components)
            }
        )
    
    # ==================== MAIN TRANSFORMATION METHOD ====================
    async def transform_all_documents(
        self, 
        query_results: Dict[str, List[Dict[str, Any]]]
    ) -> List[TransformedDocument]:
        """
        Transform query results into semantically chunked documents optimized for RAG
        
        This method creates multiple focused documents instead of a few large ones,
        making it easier for the RAG system to find relevant information.
        """
        all_documents = []
        
        # Define chunking functions and their names for logging
        chunking_functions = [
            ("User Profile", self._chunk_user_profile),
            ("Personality Analysis", self._chunk_personality_analysis),
            ("Thinking Skills", self._chunk_thinking_skills),
            ("Career Recommendations", self._chunk_career_recommendations),
            ("Competency Analysis", self._chunk_competency_analysis),
            ("Learning Style", self._chunk_learning_style),
            ("Preference Analysis", self._chunk_preference_analysis),
        ]
        
        # Execute each chunking function
        for chunk_name, chunk_function in chunking_functions:
            try:
                logger.info(f"Processing {chunk_name} documents...")
                documents = chunk_function(query_results)
                
                # â–¼â–¼â–¼ [í•µì‹¬ ì¶”ê°€] ìƒì„±ëœ ëª¨ë“  ë¬¸ì„œì— ëŒ€í•´ ê°€ìƒ ì§ˆë¬¸ì„ ìƒì„±í•˜ê³  ë©”íƒ€ë°ì´í„°ì— ì¶”ê°€ â–¼â–¼â–¼
                for doc in documents:
                    hypothetical_questions = self._generate_hypothetical_questions(
                        doc.summary_text, doc.doc_type, doc.content
                    )
                    doc.metadata['hypothetical_questions'] = hypothetical_questions
                    
                    # ê²€ìƒ‰ì— ì‚¬ìš©ë  í…ìŠ¤íŠ¸ëŠ” ì´ì œ "ìš”ì•½ë¬¸ + ê°€ìƒì§ˆë¬¸ë“¤" ì´ ë©ë‹ˆë‹¤.
                    searchable_text = doc.summary_text + "\n" + "\n".join(hypothetical_questions)
                    doc.metadata['searchable_text'] = searchable_text
                # â–²â–²â–² [í•µì‹¬ ì¶”ê°€ ë] â–²â–²â–²
                
                all_documents.extend(documents)
                logger.info(f"Created {len(documents)} {chunk_name} documents with hypothetical questions")
            except Exception as e:
                logger.error(f"Error processing {chunk_name}: {e}", exc_info=True)
                continue
        
        logger.info(f"Document transformation and chunking completed. Created {len(all_documents)} total documents.")
        
        # Log document type distribution for debugging
        doc_type_counts = defaultdict(int)
        for doc in all_documents:
            doc_type_counts[doc.doc_type] += 1
        
        logger.info(f"Document distribution: {dict(doc_type_counts)}")
        
        return all_documents