#!/usr/bin/env python3
"""
ì™„ì „í•œ ETL íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ (ì‹¤ì œ ë°ì´í„° í¬í•¨)
"""
import asyncio
import logging
from etl.legacy_query_executor import LegacyQueryExecutor
from etl.document_transformer import DocumentTransformer
from database.connection import get_async_session

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(name)s - %(message)s')
logger = logging.getLogger(__name__)

async def test_complete_etl_pipeline():
    """ì™„ì „í•œ ETL íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸"""
    anp_seq = 18240  # ì‹¤ì œ ë°ì´í„°ê°€ ìˆëŠ” ì‚¬ìš©ì
    
    logger.info("ğŸš€ Starting Complete ETL Pipeline Test")
    logger.info(f"Testing with anp_seq: {anp_seq} (ì‹¤ì œ ë°ì´í„° í¬í•¨)")
    
    try:
        # 1. Legacy Query Execution í…ŒìŠ¤íŠ¸
        logger.info("ğŸ“Š Step 1: Executing legacy queries...")
        
        session = await get_async_session()
        try:
            executor = LegacyQueryExecutor()
            results = await executor.execute_all_queries_async(session, anp_seq)
        finally:
            await session.close()
        
        # ê²°ê³¼ ìš”ì•½
        total_queries = len(results)
        successful_queries = sum(1 for result in results.values() if result.success)
        failed_queries = total_queries - successful_queries
        
        logger.info(f"Query execution completed: {successful_queries}/{total_queries} successful")
        
        # ì‹¤ì œ ë°ì´í„°ê°€ ìˆëŠ” ì¿¼ë¦¬ë“¤ í™•ì¸
        data_rich_queries = []
        for query_name, result in results.items():
            if result.success and result.data and len(result.data) > 0:
                data_rich_queries.append(f"{query_name}({len(result.data)}ê±´)")
        
        logger.info(f"ë°ì´í„°ê°€ ìˆëŠ” ì¿¼ë¦¬ë“¤: {', '.join(data_rich_queries[:10])}...")
        
        # 2. Document Transformation í…ŒìŠ¤íŠ¸
        logger.info("ğŸ“ Step 2: Transforming documents with real data...")
        
        # ê²°ê³¼ë¥¼ DocumentTransformerê°€ ê¸°ëŒ€í•˜ëŠ” í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        formatted_results = {}
        for query_name, result in results.items():
            if result.success and result.data:
                formatted_results[query_name] = result.data
            else:
                formatted_results[query_name] = []
        
        transformer = DocumentTransformer()
        documents = await transformer.transform_all_documents(formatted_results)
        
        # ë¬¸ì„œ ë¶„ì„
        doc_types = set()
        doc_type_counts = {}
        real_data_docs = 0
        
        for doc in documents:
            doc_types.add(doc.doc_type)
            doc_type_counts[doc.doc_type] = doc_type_counts.get(doc.doc_type, 0) + 1
            
            # ì‹¤ì œ ë°ì´í„°ê°€ í¬í•¨ëœ ë¬¸ì„œì¸ì§€ í™•ì¸
            if not ("ë°ì´í„°ê°€ ì•„ì§ ì¤€ë¹„ë˜ì§€" in str(doc.content) or "ë°ì´í„° ì¤€ë¹„ ì¤‘" in doc.summary_text):
                real_data_docs += 1
        
        logger.info("âœ… ETL Pipeline Test completed successfully!")
        logger.info("ğŸ“Š Final Results:")
        logger.info(f"- Total queries executed: {total_queries}")
        logger.info(f"- Successful queries: {successful_queries}")
        logger.info(f"- Failed queries: {failed_queries}")
        logger.info(f"- Documents created: {len(documents)}")
        logger.info(f"- Document types: {len(doc_types)}")
        logger.info(f"- Documents with real data: {real_data_docs}")
        logger.info(f"- Document type distribution: {doc_type_counts}")
        
        # ì˜ˆìƒë˜ëŠ” 7ê°€ì§€ ë¬¸ì„œ íƒ€ì… í™•ì¸
        expected_types = {
            'USER_PROFILE', 'PERSONALITY_PROFILE', 'THINKING_SKILLS', 
            'CAREER_RECOMMENDATIONS', 'COMPETENCY_ANALYSIS', 'LEARNING_STYLE', 
            'PREFERENCE_ANALYSIS'
        }
        
        missing_types = expected_types - doc_types
        if missing_types:
            logger.warning(f"âš ï¸ Missing document types: {missing_types}")
        else:
            logger.info("âœ… All 7 document types successfully created!")
        
        # ê°€ìƒ ì§ˆë¬¸ ìƒì„± í™•ì¸
        docs_with_questions = sum(1 for doc in documents if doc.metadata.get('hypothetical_questions'))
        logger.info(f"- Documents with hypothetical questions: {docs_with_questions}/{len(documents)}")
        
        # ìƒ˜í”Œ ë¬¸ì„œ ë‚´ìš© ì¶œë ¥
        logger.info("\nğŸ“„ Sample Document Contents:")
        for doc_type in ['USER_PROFILE', 'PERSONALITY_PROFILE', 'THINKING_SKILLS']:
            sample_doc = next((doc for doc in documents if doc.doc_type == doc_type), None)
            if sample_doc:
                logger.info(f"\n{doc_type}:")
                logger.info(f"  Summary: {sample_doc.summary_text}")
                logger.info(f"  Content keys: {list(sample_doc.content.keys()) if isinstance(sample_doc.content, dict) else 'Non-dict content'}")
                questions = sample_doc.metadata.get('hypothetical_questions', [])
                logger.info(f"  Hypothetical questions: {questions[:2]}...")
        
        return {
            "status": "success",
            "total_queries": total_queries,
            "successful_queries": successful_queries,
            "documents_created": len(documents),
            "document_types": len(doc_types),
            "real_data_documents": real_data_docs,
            "document_type_distribution": doc_type_counts,
            "missing_types": list(missing_types),
            "docs_with_questions": docs_with_questions
        }
        
    except Exception as e:
        logger.error(f"âŒ ETL Pipeline Test failed: {e}", exc_info=True)
        raise

if __name__ == "__main__":
    asyncio.run(test_complete_etl_pipeline())