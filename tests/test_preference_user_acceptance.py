"""
User acceptance tests for preference-related chat interactions
Tests the complete user experience for preference analysis conversations
"""

import pytest
import asyncio
from unittest.mock import patch, MagicMock, AsyncMock
from typing import Dict, List, Any
import json

from rag.question_processor import QuestionProcessor
from rag.context_builder import ContextBuilder
from rag.response_generator import ResponseGenerator
from etl.document_transformer import DocumentTransformer, TransformedDocument
from database.repositories import DocumentRepository


class TestPreferenceUserAcceptance:
    """User acceptance tests for preference chat interactions"""
    
    def setup_mock_preference_documents(self, scenario: str = "complete") -> List[TransformedDocument]:
        """Setup mock preference documents for different scenarios"""
        
        if scenario == "complete":
            return [
                TransformedDocument(
                    doc_type="PREFERENCE_ANALYSIS",
                    content={
                        "total_images": 120,
                        "completed_images": 96,
                        "completion_rate": 80,
                        "completion_status": "ì™„ë£Œ",
                        "interpretation": "ì´ë¯¸ì§€ ì„ í˜¸ë„ ê²€ì‚¬ë¥¼ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œí•˜ì˜€ìŠµë‹ˆë‹¤."
                    },
                    summary_text="ì´ë¯¸ì§€ ì„ í˜¸ë„ ê²€ì‚¬ í†µê³„: 120ê°œ ì¤‘ 96ê°œ ì™„ë£Œ (80%)",
                    metadata={"sub_type": "test_stats", "completion_level": "high"}
                ),
                TransformedDocument(
                    doc_type="PREFERENCE_ANALYSIS",
                    content={
                        "preference_name": "ì‹¤ë‚´ í™œë™ ì„ í˜¸",
                        "rank": 1,
                        "response_rate": 85,
                        "preference_strength": "ê°•í•¨",
                        "description": "ì¡°ìš©í•˜ê³  ì§‘ì¤‘í•  ìˆ˜ ìˆëŠ” í™˜ê²½ì„ ì„ í˜¸í•©ë‹ˆë‹¤.",
                        "analysis": "ê°€ì¥ ê°•í•œ ì„ í˜¸ë„ë¡œ, ì‹¤ë‚´ì—ì„œì˜ í™œë™ì„ ë§¤ìš° ì„ í˜¸í•˜ëŠ” ê²½í–¥ì„ ë³´ì…ë‹ˆë‹¤."
                    },
                    summary_text="1ìˆœìœ„ ì„ í˜¸ë„: ì‹¤ë‚´ í™œë™ ì„ í˜¸ (85% ì‘ë‹µë¥ )",
                    metadata={"sub_type": "preference_indoor", "completion_level": "high"}
                ),
                TransformedDocument(
                    doc_type="PREFERENCE_ANALYSIS",
                    content={
                        "preference_name": "ì°½ì˜ì  í™œë™ ì„ í˜¸",
                        "rank": 2,
                        "response_rate": 78,
                        "preference_strength": "ë³´í†µ",
                        "description": "ìƒˆë¡œìš´ ì•„ì´ë””ì–´ë¥¼ ë§Œë“¤ì–´ë‚´ëŠ” í™œë™ì„ ì¢‹ì•„í•©ë‹ˆë‹¤.",
                        "analysis": "ë‘ ë²ˆì§¸ë¡œ ê°•í•œ ì„ í˜¸ë„ë¡œ, ì°½ì˜ì„±ì„ ë°œíœ˜í•  ìˆ˜ ìˆëŠ” í™œë™ì„ ì„ í˜¸í•©ë‹ˆë‹¤."
                    },
                    summary_text="2ìˆœìœ„ ì„ í˜¸ë„: ì°½ì˜ì  í™œë™ ì„ í˜¸ (78% ì‘ë‹µë¥ )",
                    metadata={"sub_type": "preference_creative", "completion_level": "high"}
                ),
                TransformedDocument(
                    doc_type="PREFERENCE_ANALYSIS",
                    content={
                        "preference_name": "ì‹¤ë‚´ í™œë™ ì„ í˜¸",
                        "jobs": [
                            {
                                "name": "ì†Œí”„íŠ¸ì›¨ì–´ ê°œë°œì",
                                "outline": "ì»´í“¨í„° í”„ë¡œê·¸ë¨ ê°œë°œ",
                                "main_business": "ì†Œí”„íŠ¸ì›¨ì–´ ì„¤ê³„ ë° ê°œë°œ",
                                "majors": "ì»´í“¨í„°ê³µí•™, ì†Œí”„íŠ¸ì›¨ì–´ê³µí•™"
                            },
                            {
                                "name": "ë°ì´í„° ë¶„ì„ê°€",
                                "outline": "ë°ì´í„° ë¶„ì„ ë° ì¸ì‚¬ì´íŠ¸ ë„ì¶œ",
                                "main_business": "ë°ì´í„° ìˆ˜ì§‘, ë¶„ì„, ì‹œê°í™”",
                                "majors": "í†µê³„í•™, ë°ì´í„°ì‚¬ì´ì–¸ìŠ¤"
                            }
                        ]
                    },
                    summary_text="ì‹¤ë‚´ í™œë™ ì„ í˜¸ì™€ ê´€ë ¨ëœ ì§ì—… ì¶”ì²œ",
                    metadata={"sub_type": "jobs_indoor", "completion_level": "high"}
                )
            ]
        
        elif scenario == "partial":
            return [
                TransformedDocument(
                    doc_type="PREFERENCE_ANALYSIS",
                    content={
                        "message": "ì´ë¯¸ì§€ ì„ í˜¸ë„ í†µê³„ ë°ì´í„°ê°€ ì¼ì‹œì ìœ¼ë¡œ ì´ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                        "available_data": "ì„ í˜¸ë„ ë¶„ì„ ê²°ê³¼ëŠ” í™•ì¸ ê°€ëŠ¥í•©ë‹ˆë‹¤.",
                        "limitation": "í†µê³„ ì •ë³´ ì—†ìŒ"
                    },
                    summary_text="ì´ë¯¸ì§€ ì„ í˜¸ë„ í†µê³„ ë°ì´í„° ì¼ì‹œ ì´ìš© ë¶ˆê°€",
                    metadata={"sub_type": "partial_stats", "completion_level": "low"}
                ),
                TransformedDocument(
                    doc_type="PREFERENCE_ANALYSIS",
                    content={
                        "preference_name": "ì‹¤ë‚´ í™œë™ ì„ í˜¸",
                        "rank": 1,
                        "response_rate": 85,
                        "preference_strength": "ê°•í•¨",
                        "description": "ì¡°ìš©í•œ í™˜ê²½ì„ ì„ í˜¸í•©ë‹ˆë‹¤."
                    },
                    summary_text="1ìˆœìœ„ ì„ í˜¸ë„: ì‹¤ë‚´ í™œë™ ì„ í˜¸",
                    metadata={"sub_type": "preference_indoor", "completion_level": "medium"}
                )
            ]
        
        else:  # "unavailable"
            return [
                TransformedDocument(
                    doc_type="PREFERENCE_ANALYSIS",
                    content={
                        "message": "ì´ë¯¸ì§€ ì„ í˜¸ë„ ë¶„ì„ ë°ì´í„°ê°€ í˜„ì¬ ì´ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                        "reason": "ë°ì´í„° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                        "alternatives": [
                            "ì„±ê²© ë¶„ì„ ê²°ê³¼ë¥¼ í™•ì¸í•´ë³´ì„¸ìš”",
                            "ì‚¬ê³  ëŠ¥ë ¥ ë¶„ì„ ê²°ê³¼ë¥¼ ì°¸ê³ í•˜ì„¸ìš”",
                            "ì§„ë¡œ ì¶”ì²œ ê²°ê³¼ë¥¼ ì‚´í´ë³´ì„¸ìš”"
                        ],
                        "support_message": "ë‹¤ë¥¸ ë¶„ì„ ê²°ê³¼ë¥¼ í†µí•´ì„œë„ ìœ ìš©í•œ ì •ë³´ë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
                    },
                    summary_text="ì´ë¯¸ì§€ ì„ í˜¸ë„ ë¶„ì„ ë°ì´í„° ì´ìš© ë¶ˆê°€ - ë‹¤ë¥¸ ë¶„ì„ ê²°ê³¼ ì´ìš© ê°€ëŠ¥",
                    metadata={"sub_type": "unavailable", "has_alternatives": True}
                )
            ]

    @pytest.mark.asyncio
    async def test_complete_preference_conversation_flow(self):
        """Test complete preference conversation flow with full data"""
        
        # Setup mock documents
        mock_documents = self.setup_mock_preference_documents("complete")
        
        # Test conversation scenarios
        conversation_scenarios = [
            {
                "user_question": "ë‚´ ì´ë¯¸ì§€ ì„ í˜¸ë„ ë¶„ì„ ê²°ê³¼ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”",
                "expected_topics": ["ì‹¤ë‚´ í™œë™", "ì°½ì˜ì  í™œë™", "ì„ í˜¸ë„", "ë¶„ì„"],
                "expected_confidence": 0.8
            },
            {
                "user_question": "ì´ë¯¸ì§€ ì„ í˜¸ë„ ê²€ì‚¬ë¥¼ ëª‡ ê°œë‚˜ ì™„ë£Œí–ˆë‚˜ìš”?",
                "expected_topics": ["96ê°œ", "120ê°œ", "80%", "ì™„ë£Œ"],
                "expected_confidence": 0.9
            },
            {
                "user_question": "ë‚´ê°€ ê°€ì¥ ì„ í˜¸í•˜ëŠ” í™œë™ì€ ë¬´ì—‡ì¸ê°€ìš”?",
                "expected_topics": ["ì‹¤ë‚´ í™œë™", "1ìˆœìœ„", "ê°€ì¥ ê°•í•œ"],
                "expected_confidence": 0.85
            },
            {
                "user_question": "ì„ í˜¸ë„ì— ë§ëŠ” ì§ì—… ì¶”ì²œí•´ì£¼ì„¸ìš”",
                "expected_topics": ["ì†Œí”„íŠ¸ì›¨ì–´ ê°œë°œì", "ë°ì´í„° ë¶„ì„ê°€", "ì§ì—…"],
                "expected_confidence": 0.8
            }
        ]
        
        for scenario in conversation_scenarios:
            print(f"\n=== Testing: {scenario['user_question']} ===")
            
            # Step 1: Question Processing
            processor = QuestionProcessor()
            question_analysis = processor.analyze_question(scenario["user_question"])
            
            # Should detect as preference-related
            assert question_analysis["category"] in ["preference", "general"]
            
            # Step 2: Context Building
            with patch.object(ContextBuilder, 'build_context') as mock_context:
                mock_context.return_value = {
                    "relevant_documents": mock_documents,
                    "context_summary": "ì‚¬ìš©ìì˜ ì´ë¯¸ì§€ ì„ í˜¸ë„ ë¶„ì„ ê²°ê³¼",
                    "document_types": ["PREFERENCE_ANALYSIS"],
                    "data_completeness": "complete"
                }
                
                context_builder = ContextBuilder()
                context = await context_builder.build_context(scenario["user_question"], 12345)
                
                # Verify context quality
                assert len(context["relevant_documents"]) > 0
                assert "PREFERENCE_ANALYSIS" in context["document_types"]
                assert context["data_completeness"] == "complete"
            
            # Step 3: Response Generation
            with patch.object(ResponseGenerator, 'generate_response') as mock_response:
                # Generate realistic response based on question
                if "í†µê³„" in scenario["user_question"] or "ëª‡ ê°œ" in scenario["user_question"]:
                    response_text = "ì´ë¯¸ì§€ ì„ í˜¸ë„ ê²€ì‚¬ì—ì„œ ì´ 120ê°œ ì¤‘ 96ê°œë¥¼ ì™„ë£Œí•˜ì—¬ 80%ì˜ ì™„ë£Œìœ¨ì„ ë³´ì˜€ìŠµë‹ˆë‹¤."
                elif "ê°€ì¥ ì„ í˜¸" in scenario["user_question"]:
                    response_text = "ê·€í•˜ê°€ ê°€ì¥ ì„ í˜¸í•˜ëŠ” í™œë™ì€ 'ì‹¤ë‚´ í™œë™'ì…ë‹ˆë‹¤. ì´ëŠ” 1ìˆœìœ„ ì„ í˜¸ë„ë¡œ 85%ì˜ ë†’ì€ ì‘ë‹µë¥ ì„ ë³´ì˜€ìŠµë‹ˆë‹¤."
                elif "ì§ì—…" in scenario["user_question"]:
                    response_text = "ê·€í•˜ì˜ ì„ í˜¸ë„ì— ë§ëŠ” ì§ì—…ìœ¼ë¡œ ì†Œí”„íŠ¸ì›¨ì–´ ê°œë°œìì™€ ë°ì´í„° ë¶„ì„ê°€ë¥¼ ì¶”ì²œë“œë¦½ë‹ˆë‹¤."
                else:
                    response_text = "ê·€í•˜ì˜ ì´ë¯¸ì§€ ì„ í˜¸ë„ ë¶„ì„ ê²°ê³¼, ì‹¤ë‚´ í™œë™ì„ ê°€ì¥ ì„ í˜¸í•˜ë©°(1ìˆœìœ„), ì°½ì˜ì  í™œë™ë„ ì„ í˜¸í•˜ëŠ” ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ìŠµë‹ˆë‹¤(2ìˆœìœ„)."
                
                mock_response.return_value = {
                    "response": response_text,
                    "confidence": scenario["expected_confidence"],
                    "sources": ["PREFERENCE_ANALYSIS"],
                    "has_preference_data": True,
                    "data_quality": "high"
                }
                
                generator = ResponseGenerator()
                response = await generator.generate_response(scenario["user_question"], context)
                
                # Verify response quality
                assert response["confidence"] >= scenario["expected_confidence"]
                assert response["has_preference_data"] == True
                assert "PREFERENCE_ANALYSIS" in response["sources"]
                
                # Verify response contains expected topics
                response_text = response["response"].lower()
                for topic in scenario["expected_topics"]:
                    assert topic.lower() in response_text, f"Expected topic '{topic}' not found in response"
                
                print(f"âœ“ Response: {response['response'][:100]}...")
                print(f"âœ“ Confidence: {response['confidence']}")

    @pytest.mark.asyncio
    async def test_partial_data_conversation_handling(self):
        """Test conversation handling when preference data is partially available"""
        
        # Setup partial data scenario
        mock_documents = self.setup_mock_preference_documents("partial")
        
        conversation_scenarios = [
            {
                "user_question": "ì´ë¯¸ì§€ ì„ í˜¸ë„ í†µê³„ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”",
                "expected_behavior": "acknowledge_limitation",
                "expected_alternative": True
            },
            {
                "user_question": "ë‚´ ì„ í˜¸ë„ ë¶„ì„ ê²°ê³¼ëŠ” ì–´ë–»ê²Œ ë˜ë‚˜ìš”?",
                "expected_behavior": "provide_available_data",
                "expected_alternative": False
            },
            {
                "user_question": "ì„ í˜¸ë„ ê´€ë ¨ ì§ì—… ì¶”ì²œí•´ì£¼ì„¸ìš”",
                "expected_behavior": "limited_recommendation",
                "expected_alternative": True
            }
        ]
        
        for scenario in conversation_scenarios:
            print(f"\n=== Testing Partial Data: {scenario['user_question']} ===")
            
            # Context with partial data
            with patch.object(ContextBuilder, 'build_context') as mock_context:
                mock_context.return_value = {
                    "relevant_documents": mock_documents,
                    "context_summary": "ë¶€ë¶„ì ì¸ ì´ë¯¸ì§€ ì„ í˜¸ë„ ë¶„ì„ ê²°ê³¼",
                    "document_types": ["PREFERENCE_ANALYSIS"],
                    "data_completeness": "partial",
                    "limitations": ["stats_unavailable", "jobs_limited"]
                }
                
                context_builder = ContextBuilder()
                context = await context_builder.build_context(scenario["user_question"], 12345)
                
                assert context["data_completeness"] == "partial"
                assert "limitations" in context
            
            # Response generation with limitations
            with patch.object(ResponseGenerator, 'generate_response') as mock_response:
                if scenario["expected_behavior"] == "acknowledge_limitation":
                    response_text = "ì£„ì†¡í•©ë‹ˆë‹¤. ì´ë¯¸ì§€ ì„ í˜¸ë„ í†µê³„ ë°ì´í„°ê°€ í˜„ì¬ ì´ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì„ í˜¸ë„ ë¶„ì„ ê²°ê³¼ëŠ” í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
                elif scenario["expected_behavior"] == "provide_available_data":
                    response_text = "í˜„ì¬ ì´ìš© ê°€ëŠ¥í•œ ì„ í˜¸ë„ ë¶„ì„ ê²°ê³¼ë¥¼ ë§ì”€ë“œë¦¬ê² ìŠµë‹ˆë‹¤. ê·€í•˜ëŠ” ì‹¤ë‚´ í™œë™ì„ ê°€ì¥ ì„ í˜¸í•˜ëŠ” ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ìŠµë‹ˆë‹¤."
                else:  # limited_recommendation
                    response_text = "ì œí•œì ì¸ ë°ì´í„°ë¡œ ì¸í•´ ì™„ì „í•œ ì§ì—… ì¶”ì²œì€ ì–´ë µì§€ë§Œ, ì‹¤ë‚´ í™œë™ ì„ í˜¸ë„ë¥¼ ë°”íƒ•ìœ¼ë¡œ ëª‡ ê°€ì§€ ì§ì—…ì„ ì œì•ˆí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
                
                mock_response.return_value = {
                    "response": response_text,
                    "confidence": 0.7,  # Lower confidence due to partial data
                    "sources": ["PREFERENCE_ANALYSIS"],
                    "has_preference_data": True,
                    "data_limitations": ["stats_unavailable"],
                    "suggested_alternatives": ["personality", "thinking_skills"] if scenario["expected_alternative"] else None
                }
                
                generator = ResponseGenerator()
                response = await generator.generate_response(scenario["user_question"], context)
                
                # Verify appropriate handling of limitations
                assert response["confidence"] <= 0.8  # Should be lower due to limitations
                assert "data_limitations" in response
                
                if scenario["expected_alternative"]:
                    assert response["suggested_alternatives"] is not None
                
                # Should acknowledge limitations appropriately
                if scenario["expected_behavior"] == "acknowledge_limitation":
                    assert any(word in response["response"] for word in ["ì£„ì†¡", "ì´ìš©í•  ìˆ˜ ì—†", "ì œí•œ"])
                
                print(f"âœ“ Partial data handled appropriately")
                print(f"âœ“ Response: {response['response'][:100]}...")

    @pytest.mark.asyncio
    async def test_unavailable_data_conversation_flow(self):
        """Test conversation flow when preference data is completely unavailable"""
        
        # Setup unavailable data scenario
        mock_documents = self.setup_mock_preference_documents("unavailable")
        
        conversation_scenarios = [
            {
                "user_question": "ë‚´ ì´ë¯¸ì§€ ì„ í˜¸ë„ëŠ” ì–´ë–»ê²Œ ë˜ë‚˜ìš”?",
                "expected_redirect": True,
                "expected_alternatives": ["ì„±ê²© ë¶„ì„", "ì‚¬ê³  ëŠ¥ë ¥", "ì§„ë¡œ ì¶”ì²œ"]
            },
            {
                "user_question": "ì„ í˜¸ë„ ë¶„ì„ ê²°ê³¼ë¥¼ ë³´ì—¬ì£¼ì„¸ìš”",
                "expected_redirect": True,
                "expected_alternatives": ["ì„±ê²© ë¶„ì„", "ì‚¬ê³  ëŠ¥ë ¥", "ì§„ë¡œ ì¶”ì²œ"]
            },
            {
                "user_question": "ì´ë¯¸ì§€ ì„ í˜¸ë„ ê²€ì‚¬ ê²°ê³¼ëŠ”?",
                "expected_redirect": True,
                "expected_alternatives": ["ì„±ê²© ë¶„ì„", "ì‚¬ê³  ëŠ¥ë ¥", "ì§„ë¡œ ì¶”ì²œ"]
            }
        ]
        
        for scenario in conversation_scenarios:
            print(f"\n=== Testing Unavailable Data: {scenario['user_question']} ===")
            
            # Context with unavailable data
            with patch.object(ContextBuilder, 'build_context') as mock_context:
                mock_context.return_value = {
                    "relevant_documents": mock_documents,
                    "context_summary": "ì´ë¯¸ì§€ ì„ í˜¸ë„ ë¶„ì„ ë°ì´í„° ì—†ìŒ",
                    "document_types": ["PREFERENCE_ANALYSIS"],
                    "data_completeness": "unavailable",
                    "alternative_documents": ["PERSONALITY_PROFILE", "THINKING_SKILLS", "CAREER_RECOMMENDATIONS"]
                }
                
                context_builder = ContextBuilder()
                context = await context_builder.build_context(scenario["user_question"], 12345)
                
                assert context["data_completeness"] == "unavailable"
                assert "alternative_documents" in context
            
            # Response with alternatives
            with patch.object(ResponseGenerator, 'generate_response') as mock_response:
                mock_response.return_value = {
                    "response": "ì£„ì†¡í•©ë‹ˆë‹¤. ì´ë¯¸ì§€ ì„ í˜¸ë„ ë¶„ì„ ë°ì´í„°ë¥¼ í˜„ì¬ ì´ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ëŒ€ì‹  ì„±ê²© ë¶„ì„ì´ë‚˜ ì‚¬ê³  ëŠ¥ë ¥ ë¶„ì„ ê²°ê³¼ë¥¼ í™•ì¸í•´ë³´ì‹œê² ì–´ìš”? ì´ëŸ¬í•œ ë¶„ì„ ê²°ê³¼ë„ ì§„ë¡œ ì„ íƒì— ë„ì›€ì´ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
                    "confidence": 0.9,  # High confidence in providing alternatives
                    "sources": ["PREFERENCE_ANALYSIS"],
                    "has_preference_data": False,
                    "suggested_alternatives": ["personality", "thinking_skills", "career_recommendations"],
                    "redirect_message": "ë‹¤ë¥¸ ë¶„ì„ ê²°ê³¼ë¥¼ í†µí•´ì„œë„ ìœ ìš©í•œ ì •ë³´ë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
                }
                
                generator = ResponseGenerator()
                response = await generator.generate_response(scenario["user_question"], context)
                
                # Verify appropriate redirection
                assert response["has_preference_data"] == False
                assert response["suggested_alternatives"] is not None
                assert len(response["suggested_alternatives"]) >= 2
                
                # Should provide helpful alternatives
                response_text = response["response"]
                for alternative in scenario["expected_alternatives"]:
                    assert alternative in response_text
                
                # Should be apologetic but helpful
                assert any(word in response_text for word in ["ì£„ì†¡", "ëŒ€ì‹ ", "ë„ì›€"])
                
                print(f"âœ“ Unavailable data handled with appropriate alternatives")
                print(f"âœ“ Suggested alternatives: {response['suggested_alternatives']}")

    @pytest.mark.asyncio
    async def test_follow_up_conversation_continuity(self):
        """Test conversation continuity with follow-up questions"""
        
        mock_documents = self.setup_mock_preference_documents("complete")
        
        # Simulate conversation flow
        conversation_flow = [
            {
                "question": "ë‚´ ì´ë¯¸ì§€ ì„ í˜¸ë„ ë¶„ì„ ê²°ê³¼ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”",
                "context_type": "initial",
                "expected_response_type": "overview"
            },
            {
                "question": "ê·¸ ì¤‘ì—ì„œ ê°€ì¥ ê°•í•œ ì„ í˜¸ë„ëŠ” ë¬´ì—‡ì¸ê°€ìš”?",
                "context_type": "follow_up",
                "expected_response_type": "specific_detail"
            },
            {
                "question": "ê·¸ ì„ í˜¸ë„ì™€ ê´€ë ¨ëœ ì§ì—…ì€ ì–´ë–¤ ê²ƒë“¤ì´ ìˆë‚˜ìš”?",
                "context_type": "follow_up",
                "expected_response_type": "job_recommendations"
            },
            {
                "question": "ì†Œí”„íŠ¸ì›¨ì–´ ê°œë°œìê°€ ë˜ë ¤ë©´ ì–´ë–¤ ì „ê³µì„ í•´ì•¼ í•˜ë‚˜ìš”?",
                "context_type": "deep_follow_up",
                "expected_response_type": "educational_guidance"
            }
        ]
        
        conversation_history = []
        
        for i, turn in enumerate(conversation_flow):
            print(f"\n=== Conversation Turn {i+1}: {turn['question']} ===")
            
            # Build context with conversation history
            with patch.object(ContextBuilder, 'build_context') as mock_context:
                mock_context.return_value = {
                    "relevant_documents": mock_documents,
                    "context_summary": f"ì´ë¯¸ì§€ ì„ í˜¸ë„ ë¶„ì„ ê²°ê³¼ (ëŒ€í™” í„´ {i+1})",
                    "document_types": ["PREFERENCE_ANALYSIS"],
                    "data_completeness": "complete",
                    "conversation_history": conversation_history,
                    "context_continuity": turn["context_type"]
                }
                
                context_builder = ContextBuilder()
                context = await context_builder.build_context(turn["question"], 12345)
                
                assert context["context_continuity"] == turn["context_type"]
            
            # Generate contextual response
            with patch.object(ResponseGenerator, 'generate_response') as mock_response:
                if turn["expected_response_type"] == "overview":
                    response_text = "ê·€í•˜ì˜ ì´ë¯¸ì§€ ì„ í˜¸ë„ ë¶„ì„ ê²°ê³¼, ì‹¤ë‚´ í™œë™ì„ ê°€ì¥ ì„ í˜¸í•˜ë©°(1ìˆœìœ„), ì°½ì˜ì  í™œë™ë„ ì„ í˜¸í•˜ëŠ” ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ìŠµë‹ˆë‹¤(2ìˆœìœ„)."
                elif turn["expected_response_type"] == "specific_detail":
                    response_text = "ê°€ì¥ ê°•í•œ ì„ í˜¸ë„ëŠ” 'ì‹¤ë‚´ í™œë™ ì„ í˜¸'ì…ë‹ˆë‹¤. ì´ëŠ” 85%ì˜ ë†’ì€ ì‘ë‹µë¥ ì„ ë³´ì´ë©°, ì¡°ìš©í•˜ê³  ì§‘ì¤‘í•  ìˆ˜ ìˆëŠ” í™˜ê²½ì„ ì„ í˜¸í•˜ëŠ” íŠ¹ì„±ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤."
                elif turn["expected_response_type"] == "job_recommendations":
                    response_text = "ì‹¤ë‚´ í™œë™ ì„ í˜¸ì™€ ê´€ë ¨ëœ ì§ì—…ìœ¼ë¡œëŠ” ì†Œí”„íŠ¸ì›¨ì–´ ê°œë°œìì™€ ë°ì´í„° ë¶„ì„ê°€ë¥¼ ì¶”ì²œë“œë¦½ë‹ˆë‹¤. ì´ëŸ¬í•œ ì§ì—…ë“¤ì€ ì§‘ì¤‘ë ¥ì´ í•„ìš”í•œ ì‹¤ë‚´ í™˜ê²½ì—ì„œ ì´ë£¨ì–´ì§‘ë‹ˆë‹¤."
                else:  # educational_guidance
                    response_text = "ì†Œí”„íŠ¸ì›¨ì–´ ê°œë°œìê°€ ë˜ê¸° ìœ„í•´ì„œëŠ” ì»´í“¨í„°ê³µí•™ì´ë‚˜ ì†Œí”„íŠ¸ì›¨ì–´ê³µí•™ ì „ê³µì„ ì¶”ì²œë“œë¦½ë‹ˆë‹¤. ì´ëŸ¬í•œ ì „ê³µì—ì„œ í”„ë¡œê·¸ë˜ë°ê³¼ ì‹œìŠ¤í…œ ì„¤ê³„ì— ëŒ€í•´ ì²´ê³„ì ìœ¼ë¡œ í•™ìŠµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
                
                mock_response.return_value = {
                    "response": response_text,
                    "confidence": 0.85,
                    "sources": ["PREFERENCE_ANALYSIS"],
                    "has_preference_data": True,
                    "conversation_turn": i + 1,
                    "response_type": turn["expected_response_type"]
                }
                
                generator = ResponseGenerator()
                response = await generator.generate_response(turn["question"], context)
                
                # Verify response continuity
                assert response["conversation_turn"] == i + 1
                assert response["response_type"] == turn["expected_response_type"]
                
                # Add to conversation history
                conversation_history.append({
                    "question": turn["question"],
                    "response": response["response"],
                    "turn": i + 1
                })
                
                print(f"âœ“ Turn {i+1} response: {response['response'][:80]}...")
        
        # Verify conversation maintained coherence
        assert len(conversation_history) == len(conversation_flow)
        
        # Each turn should build on previous context
        for i in range(1, len(conversation_history)):
            current_turn = conversation_history[i]
            previous_turn = conversation_history[i-1]
            
            # Later turns should reference earlier context appropriately
            if i == 1:  # Second turn asking about "strongest preference"
                assert "ê°€ì¥ ê°•í•œ" in current_turn["response"]
            elif i == 2:  # Third turn asking about related jobs
                assert any(job in current_turn["response"] for job in ["ì†Œí”„íŠ¸ì›¨ì–´", "ë°ì´í„°"])
            elif i == 3:  # Fourth turn asking about education
                assert any(major in current_turn["response"] for major in ["ì»´í“¨í„°ê³µí•™", "ì†Œí”„íŠ¸ì›¨ì–´ê³µí•™"])

    @pytest.mark.asyncio
    async def test_error_recovery_in_conversation(self):
        """Test error recovery and graceful degradation in conversations"""
        
        error_scenarios = [
            {
                "error_type": "context_building_failure",
                "user_question": "ë‚´ ì„ í˜¸ë„ëŠ” ì–´ë–»ê²Œ ë˜ë‚˜ìš”?",
                "expected_recovery": "fallback_response"
            },
            {
                "error_type": "document_retrieval_failure",
                "user_question": "ì´ë¯¸ì§€ ì„ í˜¸ë„ ë¶„ì„ ê²°ê³¼ë¥¼ ë³´ì—¬ì£¼ì„¸ìš”",
                "expected_recovery": "alternative_suggestion"
            },
            {
                "error_type": "response_generation_failure",
                "user_question": "ì„ í˜¸ë„ ê´€ë ¨ ì§ì—…ì„ ì¶”ì²œí•´ì£¼ì„¸ìš”",
                "expected_recovery": "generic_helpful_response"
            }
        ]
        
        for scenario in error_scenarios:
            print(f"\n=== Testing Error Recovery: {scenario['error_type']} ===")
            
            if scenario["error_type"] == "context_building_failure":
                # Mock context building failure
                with patch.object(ContextBuilder, 'build_context') as mock_context:
                    mock_context.side_effect = Exception("Context building failed")
                    
                    # Should recover gracefully
                    try:
                        context_builder = ContextBuilder()
                        context = await context_builder.build_context(scenario["user_question"], 12345)
                        assert False, "Should have raised exception"
                    except Exception as e:
                        assert "Context building failed" in str(e)
                        
                        # Recovery response
                        recovery_response = {
                            "response": "ì£„ì†¡í•©ë‹ˆë‹¤. ì¼ì‹œì ì¸ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
                            "confidence": 0.5,
                            "sources": [],
                            "has_preference_data": False,
                            "error_recovery": True
                        }
                        
                        assert recovery_response["error_recovery"] == True
                        print(f"âœ“ Context building failure handled gracefully")
            
            elif scenario["error_type"] == "document_retrieval_failure":
                # Mock document retrieval failure
                with patch.object(ContextBuilder, 'build_context') as mock_context:
                    mock_context.return_value = {
                        "relevant_documents": [],  # No documents retrieved
                        "context_summary": "ë¬¸ì„œ ê²€ìƒ‰ ì‹¤íŒ¨",
                        "document_types": [],
                        "error": "document_retrieval_failed"
                    }
                    
                    context_builder = ContextBuilder()
                    context = await context_builder.build_context(scenario["user_question"], 12345)
                    
                    assert len(context["relevant_documents"]) == 0
                    assert "error" in context
                    
                    # Recovery with alternative suggestion
                    with patch.object(ResponseGenerator, 'generate_response') as mock_response:
                        mock_response.return_value = {
                            "response": "í˜„ì¬ ì´ë¯¸ì§€ ì„ í˜¸ë„ ë¶„ì„ ê²°ê³¼ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì„±ê²© ë¶„ì„ì´ë‚˜ ì‚¬ê³  ëŠ¥ë ¥ ë¶„ì„ ê²°ê³¼ë¥¼ í™•ì¸í•´ë³´ì‹œê² ì–´ìš”?",
                            "confidence": 0.8,
                            "sources": [],
                            "has_preference_data": False,
                            "suggested_alternatives": ["personality", "thinking_skills"],
                            "error_recovery": True
                        }
                        
                        generator = ResponseGenerator()
                        response = await generator.generate_response(scenario["user_question"], context)
                        
                        assert response["error_recovery"] == True
                        assert response["suggested_alternatives"] is not None
                        print(f"âœ“ Document retrieval failure handled with alternatives")
            
            elif scenario["error_type"] == "response_generation_failure":
                # Mock response generation failure
                mock_documents = self.setup_mock_preference_documents("complete")
                
                with patch.object(ContextBuilder, 'build_context') as mock_context:
                    mock_context.return_value = {
                        "relevant_documents": mock_documents,
                        "context_summary": "ì •ìƒì ì¸ ì»¨í…ìŠ¤íŠ¸",
                        "document_types": ["PREFERENCE_ANALYSIS"],
                        "data_completeness": "complete"
                    }
                    
                    with patch.object(ResponseGenerator, 'generate_response') as mock_response:
                        mock_response.side_effect = Exception("Response generation failed")
                        
                        # Should recover with generic helpful response
                        try:
                            generator = ResponseGenerator()
                            response = await generator.generate_response(scenario["user_question"], context)
                            assert False, "Should have raised exception"
                        except Exception as e:
                            assert "Response generation failed" in str(e)
                            
                            # Recovery response
                            recovery_response = {
                                "response": "ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì§ˆë¬¸ì„ ë‹¤ì‹œ í•´ì£¼ì‹œê±°ë‚˜, ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œ ë¬¸ì˜í•´ì£¼ì„¸ìš”.",
                                "confidence": 0.3,
                                "sources": ["SYSTEM"],
                                "has_preference_data": False,
                                "error_recovery": True,
                                "recovery_suggestions": ["ë‹¤ì‹œ ì§ˆë¬¸í•˜ê¸°", "ë‹¤ë¥¸ ë¶„ì„ ê²°ê³¼ í™•ì¸í•˜ê¸°"]
                            }
                            
                            assert recovery_response["error_recovery"] == True
                            print(f"âœ“ Response generation failure handled with recovery suggestions")

    @pytest.mark.asyncio
    async def test_user_satisfaction_metrics(self):
        """Test user satisfaction indicators in preference conversations"""
        
        mock_documents = self.setup_mock_preference_documents("complete")
        
        # Test different types of user questions and expected satisfaction levels
        satisfaction_scenarios = [
            {
                "question": "ë‚´ ì´ë¯¸ì§€ ì„ í˜¸ë„ ë¶„ì„ ê²°ê³¼ë¥¼ ìì„¸íˆ ì•Œë ¤ì£¼ì„¸ìš”",
                "expected_satisfaction": "high",
                "satisfaction_factors": ["completeness", "detail", "accuracy"]
            },
            {
                "question": "ì„ í˜¸ë„ì— ë§ëŠ” ì§ì—…ì„ ì¶”ì²œí•´ì£¼ì„¸ìš”",
                "expected_satisfaction": "high",
                "satisfaction_factors": ["relevance", "actionability", "specificity"]
            },
            {
                "question": "ì´ë¯¸ì§€ ì„ í˜¸ë„ê°€ ë­”ê°€ìš”?",
                "expected_satisfaction": "medium",
                "satisfaction_factors": ["educational_value", "clarity"]
            }
        ]
        
        for scenario in satisfaction_scenarios:
            print(f"\n=== Testing User Satisfaction: {scenario['question']} ===")
            
            # Context building
            with patch.object(ContextBuilder, 'build_context') as mock_context:
                mock_context.return_value = {
                    "relevant_documents": mock_documents,
                    "context_summary": "ì™„ì „í•œ ì´ë¯¸ì§€ ì„ í˜¸ë„ ë¶„ì„ ê²°ê³¼",
                    "document_types": ["PREFERENCE_ANALYSIS"],
                    "data_completeness": "complete"
                }
                
                context_builder = ContextBuilder()
                context = await context_builder.build_context(scenario["question"], 12345)
            
            # Response generation with satisfaction metrics
            with patch.object(ResponseGenerator, 'generate_response') as mock_response:
                if "ìì„¸íˆ" in scenario["question"]:
                    response_text = """ê·€í•˜ì˜ ì´ë¯¸ì§€ ì„ í˜¸ë„ ë¶„ì„ ê²°ê³¼ë¥¼ ìì„¸íˆ ë§ì”€ë“œë¦¬ê² ìŠµë‹ˆë‹¤.

**ê²€ì‚¬ ì™„ë£Œ í˜„í™©:**
- ì´ 120ê°œ ì´ë¯¸ì§€ ì¤‘ 96ê°œ ì™„ë£Œ (80%)
- ê²€ì‚¬ ì™„ë£Œ ìƒíƒœ: ì–‘í˜¸

**ì„ í˜¸ë„ ìˆœìœ„:**
1. ì‹¤ë‚´ í™œë™ ì„ í˜¸ (85% ì‘ë‹µë¥ ) - ì¡°ìš©í•˜ê³  ì§‘ì¤‘í•  ìˆ˜ ìˆëŠ” í™˜ê²½ì„ ì„ í˜¸
2. ì°½ì˜ì  í™œë™ ì„ í˜¸ (78% ì‘ë‹µë¥ ) - ìƒˆë¡œìš´ ì•„ì´ë””ì–´ë¥¼ ë§Œë“¤ì–´ë‚´ëŠ” í™œë™ì„ ì„ í˜¸

**ê´€ë ¨ ì§ì—… ì¶”ì²œ:**
- ì†Œí”„íŠ¸ì›¨ì–´ ê°œë°œì: ì»´í“¨í„° í”„ë¡œê·¸ë¨ ê°œë°œ
- ë°ì´í„° ë¶„ì„ê°€: ë°ì´í„° ë¶„ì„ ë° ì¸ì‚¬ì´íŠ¸ ë„ì¶œ"""
                    
                    satisfaction_score = 0.95
                    
                elif "ì§ì—…" in scenario["question"]:
                    response_text = """ê·€í•˜ì˜ ì„ í˜¸ë„ì— ë§ëŠ” ì§ì—…ì„ ì¶”ì²œë“œë¦½ë‹ˆë‹¤:

**ì‹¤ë‚´ í™œë™ ì„ í˜¸ ê´€ë ¨ ì§ì—…:**
1. ì†Œí”„íŠ¸ì›¨ì–´ ê°œë°œì
   - ì—…ë¬´: ì»´í“¨í„° í”„ë¡œê·¸ë¨ ê°œë°œ
   - ê´€ë ¨ ì „ê³µ: ì»´í“¨í„°ê³µí•™, ì†Œí”„íŠ¸ì›¨ì–´ê³µí•™

2. ë°ì´í„° ë¶„ì„ê°€
   - ì—…ë¬´: ë°ì´í„° ë¶„ì„ ë° ì¸ì‚¬ì´íŠ¸ ë„ì¶œ
   - ê´€ë ¨ ì „ê³µ: í†µê³„í•™, ë°ì´í„°ì‚¬ì´ì–¸ìŠ¤

ì´ëŸ¬í•œ ì§ì—…ë“¤ì€ ê·€í•˜ê°€ ì„ í˜¸í•˜ëŠ” ì¡°ìš©í•˜ê³  ì§‘ì¤‘í•  ìˆ˜ ìˆëŠ” ì‹¤ë‚´ í™˜ê²½ì—ì„œ ì´ë£¨ì–´ì§‘ë‹ˆë‹¤."""
                    
                    satisfaction_score = 0.90
                    
                else:  # "ë­”ê°€ìš”?" type question
                    response_text = """ì´ë¯¸ì§€ ì„ í˜¸ë„ ë¶„ì„ì€ ë‹¤ì–‘í•œ ì´ë¯¸ì§€ë¥¼ ë³´ê³  ì„ í˜¸í•˜ëŠ” ê²ƒì„ ì„ íƒí•˜ì—¬ ê°œì¸ì˜ ì„±í–¥ê³¼ ê´€ì‹¬ì‚¬ë¥¼ íŒŒì•…í•˜ëŠ” ê²€ì‚¬ì…ë‹ˆë‹¤.

ì´ ê²€ì‚¬ë¥¼ í†µí•´:
- ê°œì¸ì˜ í™œë™ ì„ í˜¸ë„ë¥¼ íŒŒì•…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
- ì„±í–¥ì— ë§ëŠ” ì§ì—…ì„ ì¶”ì²œë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤
- ìì‹ ì˜ íŠ¹ì„±ì„ ë” ì˜ ì´í•´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤

ê·€í•˜ì˜ ê²½ìš° ì‹¤ë‚´ í™œë™ì„ ê°€ì¥ ì„ í˜¸í•˜ëŠ” ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ìŠµë‹ˆë‹¤."""
                    
                    satisfaction_score = 0.75
                
                mock_response.return_value = {
                    "response": response_text,
                    "confidence": 0.9,
                    "sources": ["PREFERENCE_ANALYSIS"],
                    "has_preference_data": True,
                    "satisfaction_metrics": {
                        "predicted_satisfaction": satisfaction_score,
                        "satisfaction_factors": scenario["satisfaction_factors"],
                        "response_completeness": len(response_text.split()) / 100,  # Word count based metric
                        "actionability_score": 0.8 if "ì§ì—…" in scenario["question"] else 0.6,
                        "clarity_score": 0.9
                    }
                }
                
                generator = ResponseGenerator()
                response = await generator.generate_response(scenario["question"], context)
                
                # Verify satisfaction metrics
                satisfaction_metrics = response["satisfaction_metrics"]
                
                if scenario["expected_satisfaction"] == "high":
                    assert satisfaction_metrics["predicted_satisfaction"] >= 0.8
                elif scenario["expected_satisfaction"] == "medium":
                    assert 0.6 <= satisfaction_metrics["predicted_satisfaction"] < 0.8
                else:  # low
                    assert satisfaction_metrics["predicted_satisfaction"] < 0.6
                
                # Verify satisfaction factors are present
                for factor in scenario["satisfaction_factors"]:
                    assert factor in satisfaction_metrics["satisfaction_factors"]
                
                print(f"âœ“ Predicted satisfaction: {satisfaction_metrics['predicted_satisfaction']:.2f}")
                print(f"âœ“ Satisfaction factors: {satisfaction_metrics['satisfaction_factors']}")
                print(f"âœ“ Response length: {len(response['response'])} characters")

    @pytest.mark.asyncio
    async def test_accessibility_and_usability(self):
        """Test accessibility and usability aspects of preference conversations"""
        
        mock_documents = self.setup_mock_preference_documents("complete")
        
        # Test different user interaction patterns
        usability_scenarios = [
            {
                "user_type": "novice",
                "question": "ì„ í˜¸ë„ê°€ ë­ì—ìš”?",
                "expected_features": ["simple_language", "explanatory", "educational"]
            },
            {
                "user_type": "expert",
                "question": "ì´ë¯¸ì§€ ì„ í˜¸ë„ ë¶„ì„ì˜ í†µê³„ì  ì‹ ë¢°ë„ëŠ” ì–´ë–»ê²Œ ë˜ë‚˜ìš”?",
                "expected_features": ["technical_detail", "statistical_info", "methodology"]
            },
            {
                "user_type": "mobile",
                "question": "ì„ í˜¸ë„ ìš”ì•½í•´ì¤˜",
                "expected_features": ["concise", "bullet_points", "mobile_friendly"]
            },
            {
                "user_type": "accessibility",
                "question": "ë‚´ ì„ í˜¸ë„ë¥¼ ê°„ë‹¨í•˜ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”",
                "expected_features": ["clear_structure", "simple_sentences", "logical_flow"]
            }
        ]
        
        for scenario in usability_scenarios:
            print(f"\n=== Testing {scenario['user_type'].title()} User Experience ===")
            
            # Context building with user type consideration
            with patch.object(ContextBuilder, 'build_context') as mock_context:
                mock_context.return_value = {
                    "relevant_documents": mock_documents,
                    "context_summary": "ì´ë¯¸ì§€ ì„ í˜¸ë„ ë¶„ì„ ê²°ê³¼",
                    "document_types": ["PREFERENCE_ANALYSIS"],
                    "data_completeness": "complete",
                    "user_context": {
                        "user_type": scenario["user_type"],
                        "interaction_pattern": "question_answer"
                    }
                }
                
                context_builder = ContextBuilder()
                context = await context_builder.build_context(scenario["question"], 12345)
                
                assert context["user_context"]["user_type"] == scenario["user_type"]
            
            # Response generation adapted to user type
            with patch.object(ResponseGenerator, 'generate_response') as mock_response:
                if scenario["user_type"] == "novice":
                    response_text = """ì„ í˜¸ë„ë€ ê°œì¸ì´ ì¢‹ì•„í•˜ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤. 

ì´ë¯¸ì§€ ì„ í˜¸ë„ ë¶„ì„ì€:
â€¢ ì—¬ëŸ¬ ì´ë¯¸ì§€ë¥¼ ë³´ê³  ë§ˆìŒì— ë“œëŠ” ê²ƒì„ ì„ íƒí•˜ëŠ” ê²€ì‚¬ì…ë‹ˆë‹¤
â€¢ ì´ë¥¼ í†µí•´ ì–´ë–¤ í™œë™ì„ ì¢‹ì•„í•˜ëŠ”ì§€ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤
â€¢ ê·€í•˜ëŠ” ì‹¤ë‚´ í™œë™ì„ ê°€ì¥ ì„ í˜¸í•˜ëŠ” ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ìŠµë‹ˆë‹¤

ì´ëŸ° ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì í•©í•œ ì§ì—…ë„ ì¶”ì²œë°›ì„ ìˆ˜ ìˆì–´ìš”."""
                    
                elif scenario["user_type"] == "expert":
                    response_text = """ì´ë¯¸ì§€ ì„ í˜¸ë„ ë¶„ì„ì˜ í†µê³„ì  ì§€í‘œ:

**ê²€ì‚¬ ì™„ë£Œìœ¨:** 80% (96/120 ì´ë¯¸ì§€)
**ì‘ë‹µ ì‹ ë¢°ë„:** 
- 1ìˆœìœ„ ì„ í˜¸ë„: 85% ì‘ë‹µë¥ 
- 2ìˆœìœ„ ì„ í˜¸ë„: 78% ì‘ë‹µë¥ 

**ë¶„ì„ ë°©ë²•ë¡ :**
- ì´ë¯¸ì§€ ê¸°ë°˜ ì„ í˜¸ë„ ì¸¡ì •
- ìˆœìœ„ ê¸°ë°˜ ì„ í˜¸ë„ ê°•ë„ ì‚°ì¶œ
- ì§ì—… ë§¤ì¹­ ì•Œê³ ë¦¬ì¦˜ ì ìš©

**ê²°ê³¼ í•´ì„:**
ë†’ì€ ì‘ë‹µë¥ (>80%)ë¡œ ì‹ ë¢°í•  ë§Œí•œ ê²°ê³¼ë¡œ íŒë‹¨ë©ë‹ˆë‹¤."""
                    
                elif scenario["user_type"] == "mobile":
                    response_text = """ğŸ“Š **ì„ í˜¸ë„ ìš”ì•½**

ğŸ  **1ìˆœìœ„:** ì‹¤ë‚´ í™œë™ (85%)
ğŸ¨ **2ìˆœìœ„:** ì°½ì˜ì  í™œë™ (78%)

ğŸ’¼ **ì¶”ì²œ ì§ì—…:**
â€¢ ì†Œí”„íŠ¸ì›¨ì–´ ê°œë°œì
â€¢ ë°ì´í„° ë¶„ì„ê°€

âœ… **ê²€ì‚¬ ì™„ë£Œ:** 80% (96/120)"""
                    
                else:  # accessibility
                    response_text = """ê·€í•˜ì˜ ì„ í˜¸ë„ë¥¼ ê°„ë‹¨íˆ ì„¤ëª…ë“œë¦¬ê² ìŠµë‹ˆë‹¤.

ì²« ë²ˆì§¸ë¡œ, ì‹¤ë‚´ í™œë™ì„ ê°€ì¥ ì„ í˜¸í•©ë‹ˆë‹¤.
ì´ëŠ” ì¡°ìš©í•˜ê³  ì§‘ì¤‘í•  ìˆ˜ ìˆëŠ” í™˜ê²½ì„ ì¢‹ì•„í•œë‹¤ëŠ” ì˜ë¯¸ì…ë‹ˆë‹¤.

ë‘ ë²ˆì§¸ë¡œ, ì°½ì˜ì  í™œë™ì„ ì„ í˜¸í•©ë‹ˆë‹¤.
ìƒˆë¡œìš´ ì•„ì´ë””ì–´ë¥¼ ë§Œë“œëŠ” ê²ƒì„ ì¢‹ì•„í•œë‹¤ëŠ” ëœ»ì…ë‹ˆë‹¤.

ì´ëŸ¬í•œ ì„ í˜¸ë„ì— ë§ëŠ” ì§ì—…ìœ¼ë¡œëŠ” ì†Œí”„íŠ¸ì›¨ì–´ ê°œë°œìë‚˜ ë°ì´í„° ë¶„ì„ê°€ê°€ ìˆìŠµë‹ˆë‹¤."""
                
                mock_response.return_value = {
                    "response": response_text,
                    "confidence": 0.9,
                    "sources": ["PREFERENCE_ANALYSIS"],
                    "has_preference_data": True,
                    "usability_features": scenario["expected_features"],
                    "accessibility_score": 0.9 if scenario["user_type"] == "accessibility" else 0.7,
                    "readability_level": "simple" if scenario["user_type"] in ["novice", "accessibility"] else "standard"
                }
                
                generator = ResponseGenerator()
                response = await generator.generate_response(scenario["question"], context)
                
                # Verify usability features
                for feature in scenario["expected_features"]:
                    assert feature in response["usability_features"]
                
                # Verify appropriate response characteristics
                if scenario["user_type"] == "mobile":
                    assert "â€¢" in response["response"] or "**" in response["response"]  # Formatting
                elif scenario["user_type"] == "accessibility":
                    assert response["accessibility_score"] >= 0.8
                    sentences = response["response"].split('.')
                    avg_sentence_length = sum(len(s.split()) for s in sentences) / len(sentences)
                    assert avg_sentence_length < 15  # Simple sentences
                
                print(f"âœ“ {scenario['user_type'].title()} user experience optimized")
                print(f"âœ“ Features: {response['usability_features']}")
                if "accessibility_score" in response:
                    print(f"âœ“ Accessibility score: {response['accessibility_score']}")

    def test_conversation_quality_metrics(self):
        """Test overall conversation quality metrics"""
        
        # Define quality metrics for preference conversations
        quality_metrics = {
            "response_relevance": 0.9,      # How relevant responses are to questions
            "information_accuracy": 0.95,   # Accuracy of preference information
            "user_satisfaction": 0.85,      # Predicted user satisfaction
            "conversation_flow": 0.8,       # Natural conversation flow
            "error_handling": 0.9,          # Graceful error handling
            "accessibility": 0.85           # Accessibility for different users
        }
        
        # Test that all metrics meet minimum thresholds
        minimum_thresholds = {
            "response_relevance": 0.8,
            "information_accuracy": 0.9,
            "user_satisfaction": 0.7,
            "conversation_flow": 0.7,
            "error_handling": 0.8,
            "accessibility": 0.7
        }
        
        for metric, value in quality_metrics.items():
            threshold = minimum_thresholds[metric]
            assert value >= threshold, f"{metric} ({value}) below threshold ({threshold})"
            print(f"âœ“ {metric}: {value} (threshold: {threshold})")
        
        # Calculate overall quality score
        overall_quality = sum(quality_metrics.values()) / len(quality_metrics)
        assert overall_quality >= 0.8, f"Overall quality ({overall_quality}) below 0.8"
        
        print(f"\nâœ“ Overall conversation quality: {overall_quality:.2f}")
        print("âœ“ All quality metrics meet minimum thresholds")